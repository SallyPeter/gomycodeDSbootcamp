{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTzaUvyKsACybnyZe18vIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SallyPeter/gomycodeDSbootcamp/blob/main/Python/Checkpoint_Web_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What You're Aiming For**\n",
        "\n",
        "Scraping text from Wikipedia  website using Beautiful Soup\n",
        "\n",
        "\n",
        "## **Instructions**\n",
        "\n",
        "After watching this video below, you will be able to:\n",
        "\n",
        "➡️ https://www.youtube.com/watch?v=YY5skv756pc\n",
        "\n",
        "1. Write a function to Get and parse html content from a Wikipedia page\n",
        "\n",
        "2.  Write a function to Extract article title\n",
        "\n",
        "3. Write a function to Extract article text for each paragraph with their respective headings. Map those headings to their respective paragraphs in the dictionary.\n",
        "\n",
        "4. Write a function to collect every link that redirects to another Wikipedia page\n",
        "\n",
        "5. Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
        "\n",
        "6. Test the last function on a Wikipedia page of your choice"
      ],
      "metadata": {
        "id": "_gndjdf6OU2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests --q\n",
        "!pip install beautifulsoup4 --q"
      ],
      "metadata": {
        "id": "M1m2BDWZPfGN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "WYfgHUUoPY-8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q934KBG_OCmS"
      },
      "outputs": [],
      "source": [
        "# Write a function to Get and parse html content from a Wikipedia page\n",
        "\n",
        "def get_parse(url):\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "  return soup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function to Extract article title\n",
        "\n",
        "def extract_title(soup):\n",
        "  soup.find(\"title\").get_text()\n"
      ],
      "metadata": {
        "id": "0sgEcSAAOrTB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function to Extract article text for each paragraph with their respective headings. Map those headings to their respective paragraphs in the dictionary.\n",
        "def extract_text(soup, class_name = 'mw-heading mw-heading2'):\n",
        "\n",
        "  key_name = soup.find('div', class_ = class_name ).find([\"h2\",\"h3\",\"h4\"]).get_text()\n",
        "  div_list =  soup.find('div', class_ =class_name ).find_next_siblings()\n",
        "  para_dict = {key_name: ''}\n",
        "\n",
        "  for each in div_list:\n",
        "    if each.name == 'div':\n",
        "      ddiv = each.find([\"h2\",\"h3\", \"h4\"])\n",
        "      if ddiv != None:\n",
        "        key_name = ddiv.get_text()\n",
        "      if type(key_name) == str:\n",
        "        para_dict[key_name] = ''\n",
        "    elif each.name == 'p':\n",
        "      para_dict[key_name] = para_dict[key_name] + each.get_text()\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "    para_dict[key_name] = para_dict[key_name].strip().strip('\\n')\n",
        "\n",
        "  return para_dict\n"
      ],
      "metadata": {
        "id": "D3Xi04-IOrPm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function to collect every link that redirects to another Wikipedia page\n",
        "\n",
        "def extract_link(soup):\n",
        "  link_list = []\n",
        "  for each in soup.find_all('a'):\n",
        "    # link_list.append(each['href'])\n",
        "    href = each.get('href')  # This returns None if 'href' doesn't exist\n",
        "    if href:\n",
        "      link_list.append(href)\n",
        "\n",
        "  return link_list\n"
      ],
      "metadata": {
        "id": "pBIWgAbfOrMw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
        "\n",
        "def all_(wiki_link):\n",
        "  if wiki_link.startswith('https://en.wikipedia.org/'):\n",
        "    soup = get_parse(wiki_link)\n",
        "    title = extract_title(soup)\n",
        "    all_para = extract_text(soup)\n",
        "    all_links = extract_link(soup)\n",
        "\n",
        "    print(\"**********************************************************************************************\")\n",
        "    print(title)\n",
        "    print(\"**********************************************************************************************\")\n",
        "    print(all_para)\n",
        "    print(\"**********************************************************************************************\")\n",
        "    print(all_links)\n",
        "\n",
        "  else:\n",
        "    print('Please enter a Wikipedia Link')\n",
        "\n"
      ],
      "metadata": {
        "id": "zvysXX9uOrJ7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the last function on a Wikipedia page of your choice\n",
        "\n",
        "wiki_link = input(\"Enter a wikipedia link: \")\n",
        "all_(wiki_link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I8jq3OlOrG5",
        "outputId": "7cc64802-53d4-4100-c98d-015050f08618"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a wikipedia link: https://en.wikipedia.org/wiki/Reinforcement_learning\n",
            "**********************************************************************************************\n",
            "None\n",
            "**********************************************************************************************\n",
            "{'Introduction': 'Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment.Basic reinforcement learning is modeled as a Markov decision process:The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the \"reward function\" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. (See Reinforcement.) For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]A basic reinforcement learning AI agent interacts with its environment in discrete time steps. At each time t, the agent receives the current state \\n\\n\\n\\n\\nS\\n\\nt\\n\\n\\n\\n\\n{\\\\displaystyle S_{t}}\\n\\n and reward \\n\\n\\n\\n\\nR\\n\\nt\\n\\n\\n\\n\\n{\\\\displaystyle R_{t}}\\n\\n. It then chooses an action \\n\\n\\n\\n\\nA\\n\\nt\\n\\n\\n\\n\\n{\\\\displaystyle A_{t}}\\n\\n from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \\n\\n\\n\\n\\nS\\n\\nt\\n+\\n1\\n\\n\\n\\n\\n{\\\\displaystyle S_{t+1}}\\n\\n and the reward \\n\\n\\n\\n\\nR\\n\\nt\\n+\\n1\\n\\n\\n\\n\\n{\\\\displaystyle R_{t+1}}\\n\\n associated with the transition \\n\\n\\n\\n(\\n\\nS\\n\\nt\\n\\n\\n,\\n\\nA\\n\\nt\\n\\n\\n,\\n\\nS\\n\\nt\\n+\\n1\\n\\n\\n)\\n\\n\\n{\\\\displaystyle (S_{t},A_{t},S_{t+1})}\\n\\n is determined. The goal of a reinforcement learning agent is to learn a policy: \\n\\n\\n\\nπ\\n:\\n\\n\\nS\\n\\n\\n×\\n\\n\\nA\\n\\n\\n→\\n[\\n0\\n,\\n1\\n]\\n\\n\\n{\\\\displaystyle \\\\pi :{\\\\mathcal {S}}\\\\times {\\\\mathcal {A}}\\\\rightarrow [0,1]}\\n\\n, \\n\\n\\n\\nπ\\n(\\ns\\n,\\na\\n)\\n=\\nPr\\n(\\n\\nA\\n\\nt\\n\\n\\n=\\na\\n∣\\n\\nS\\n\\nt\\n\\n\\n=\\ns\\n)\\n\\n\\n{\\\\displaystyle \\\\pi (s,a)=\\\\Pr(A_{t}=a\\\\mid S_{t}=s)}\\n\\n that maximizes the expected cumulative reward.Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.When the agent\\'s performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation,[6] robot control,[7] photovoltaic generators dispatch,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.', 'Exploration': 'The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.One such method is \\n\\n\\n\\nε\\n\\n\\n{\\\\displaystyle \\\\varepsilon }\\n\\n-greedy, where \\n\\n\\n\\n0\\n<\\nε\\n<\\n1\\n\\n\\n{\\\\displaystyle 0<\\\\varepsilon <1}\\n\\n is a parameter controlling the amount of exploration vs. exploitation.  With probability \\n\\n\\n\\n1\\n−\\nε\\n\\n\\n{\\\\displaystyle 1-\\\\varepsilon }\\n\\n, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \\n\\n\\n\\nε\\n\\n\\n{\\\\displaystyle \\\\varepsilon }\\n\\n, exploration is chosen, and the action is chosen uniformly at random. \\n\\n\\n\\nε\\n\\n\\n{\\\\displaystyle \\\\varepsilon }\\n\\n is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]', 'Algorithms for control learning': 'Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.', 'Criterion of optimality': '', 'Policy': \"The agent's action selection is modeled as a map called policy:The policy map gives the probability of taking action \\n\\n\\n\\na\\n\\n\\n{\\\\displaystyle a}\\n\\n when in state \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n.[14]:\\u200a61\\u200a There are also deterministic policies.\", 'State-value function': 'The state-value function \\n\\n\\n\\n\\nV\\n\\nπ\\n\\n\\n(\\ns\\n)\\n\\n\\n{\\\\displaystyle V_{\\\\pi }(s)}\\n\\n is defined as, expected discounted return starting with state \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n, i.e. \\n\\n\\n\\n\\nS\\n\\n0\\n\\n\\n=\\ns\\n\\n\\n{\\\\displaystyle S_{0}=s}\\n\\n, and successively following policy \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.[14]:\\u200a60where the random variable \\n\\n\\n\\nG\\n\\n\\n{\\\\displaystyle G}\\n\\n denotes the discounted return, and is defined as the sum of future discounted rewards:where \\n\\n\\n\\n\\nR\\n\\nt\\n+\\n1\\n\\n\\n\\n\\n{\\\\displaystyle R_{t+1}}\\n\\n is the reward for transitioning from state \\n\\n\\n\\n\\nS\\n\\nt\\n\\n\\n\\n\\n{\\\\displaystyle S_{t}}\\n\\n to \\n\\n\\n\\n\\nS\\n\\nt\\n+\\n1\\n\\n\\n\\n\\n{\\\\displaystyle S_{t+1}}\\n\\n, \\n\\n\\n\\n0\\n≤\\nγ\\n<\\n1\\n\\n\\n{\\\\displaystyle 0\\\\leq \\\\gamma <1}\\n\\n is the discount rate. \\n\\n\\n\\nγ\\n\\n\\n{\\\\displaystyle \\\\gamma }\\n\\n is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent\\'s history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.', 'Brute force': 'The brute force approach entails two steps:One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.', 'Value function': 'Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \\n\\n\\n\\n\\n\\nE\\n\\n\\n\\u2061\\n[\\nG\\n]\\n\\n\\n{\\\\displaystyle \\\\operatorname {\\\\mathbb {E} } [G]}\\n\\n for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.To define optimality in a formal manner, define the state-value of a policy \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n bywhere \\n\\n\\n\\nG\\n\\n\\n{\\\\displaystyle G}\\n\\n stands for the discounted return associated with following \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n from the initial state \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n. Defining \\n\\n\\n\\n\\nV\\n\\n∗\\n\\n\\n(\\ns\\n)\\n\\n\\n{\\\\displaystyle V^{*}(s)}\\n\\n as the maximum possible state-value of \\n\\n\\n\\n\\nV\\n\\nπ\\n\\n\\n(\\ns\\n)\\n\\n\\n{\\\\displaystyle V^{\\\\pi }(s)}\\n\\n, where \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n is allowed to change,A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since \\n\\n\\n\\n\\nV\\n\\n∗\\n\\n\\n(\\ns\\n)\\n=\\n\\nmax\\n\\nπ\\n\\n\\n\\nE\\n\\n[\\nG\\n∣\\ns\\n,\\nπ\\n]\\n\\n\\n{\\\\displaystyle V^{*}(s)=\\\\max _{\\\\pi }\\\\mathbb {E} [G\\\\mid s,\\\\pi ]}\\n\\n, where \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n is a state randomly sampled from the distribution \\n\\n\\n\\nμ\\n\\n\\n{\\\\displaystyle \\\\mu }\\n\\n of initial states (so \\n\\n\\n\\nμ\\n(\\ns\\n)\\n=\\nPr\\n(\\n\\nS\\n\\n0\\n\\n\\n=\\ns\\n)\\n\\n\\n{\\\\displaystyle \\\\mu (s)=\\\\Pr(S_{0}=s)}\\n\\n).Although state-values suffice to define optimality, it is useful to define action-values. Given a state \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n, an action \\n\\n\\n\\na\\n\\n\\n{\\\\displaystyle a}\\n\\n and a policy \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n, the action-value of the pair \\n\\n\\n\\n(\\ns\\n,\\na\\n)\\n\\n\\n{\\\\displaystyle (s,a)}\\n\\n under \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n is defined bywhere \\n\\n\\n\\nG\\n\\n\\n{\\\\displaystyle G}\\n\\n now stands for the random discounted return associated with first taking action \\n\\n\\n\\na\\n\\n\\n{\\\\displaystyle a}\\n\\n in state \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n and following \\n\\n\\n\\nπ\\n\\n\\n{\\\\displaystyle \\\\pi }\\n\\n, thereafter.The theory of Markov decision processes states that if \\n\\n\\n\\n\\nπ\\n\\n∗\\n\\n\\n\\n\\n{\\\\displaystyle \\\\pi ^{*}}\\n\\n is an optimal policy, we act optimally (take the optimal action) by choosing the action from \\n\\n\\n\\n\\nQ\\n\\n\\nπ\\n\\n∗\\n\\n\\n\\n\\n(\\ns\\n,\\n⋅\\n)\\n\\n\\n{\\\\displaystyle Q^{\\\\pi ^{*}}(s,\\\\cdot )}\\n\\n with the highest action-value at each state, \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n. The action-value function of such an optimal policy (\\n\\n\\n\\n\\nQ\\n\\n\\nπ\\n\\n∗\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle Q^{\\\\pi ^{*}}}\\n\\n) is called the optimal action-value function and is commonly denoted by \\n\\n\\n\\n\\nQ\\n\\n∗\\n\\n\\n\\n\\n{\\\\displaystyle Q^{*}}\\n\\n. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \\n\\n\\n\\n\\nQ\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle Q_{k}}\\n\\n (\\n\\n\\n\\nk\\n=\\n0\\n,\\n1\\n,\\n2\\n,\\n…\\n\\n\\n{\\\\displaystyle k=0,1,2,\\\\ldots }\\n\\n) that converge to \\n\\n\\n\\n\\nQ\\n\\n∗\\n\\n\\n\\n\\n{\\\\displaystyle Q^{*}}\\n\\n. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.', 'Monte Carlo methods': 'Monte Carlo methods[15] are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment’s dynamics, Monte Carlo methods rely solely on actual or simulated experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term “Monte Carlo” generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.These methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process (MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.[14]', 'Temporal difference methods': \"The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[16][17] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[18] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \\n\\n\\n\\nλ\\n\\n\\n{\\\\displaystyle \\\\lambda }\\n\\n parameter \\n\\n\\n\\n(\\n0\\n≤\\nλ\\n≤\\n1\\n)\\n\\n\\n{\\\\displaystyle (0\\\\leq \\\\lambda \\\\leq 1)}\\n\\n that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\", 'Function approximation methods': 'In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \\n\\n\\n\\nϕ\\n\\n\\n{\\\\displaystyle \\\\phi }\\n\\n that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \\n\\n\\n\\n(\\ns\\n,\\na\\n)\\n\\n\\n{\\\\displaystyle (s,a)}\\n\\n are obtained by linearly combining the components of \\n\\n\\n\\nϕ\\n(\\ns\\n,\\na\\n)\\n\\n\\n{\\\\displaystyle \\\\phi (s,a)}\\n\\n with some weights \\n\\n\\n\\nθ\\n\\n\\n{\\\\displaystyle \\\\theta }\\n\\n:The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[19] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[20]The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.', 'Direct policy search': \"An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \\n\\n\\n\\nθ\\n\\n\\n{\\\\displaystyle \\\\theta }\\n\\n, let \\n\\n\\n\\n\\nπ\\n\\nθ\\n\\n\\n\\n\\n{\\\\displaystyle \\\\pi _{\\\\theta }}\\n\\n denote the policy associated to \\n\\n\\n\\nθ\\n\\n\\n{\\\\displaystyle \\\\theta }\\n\\n. Defining the performance function by \\n\\n\\n\\nρ\\n(\\nθ\\n)\\n=\\n\\nρ\\n\\n\\nπ\\n\\nθ\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle \\\\rho (\\\\theta )=\\\\rho ^{\\\\pi _{\\\\theta }}}\\n\\n under mild conditions this function will be differentiable as a function of the parameter vector \\n\\n\\n\\nθ\\n\\n\\n{\\\\displaystyle \\\\theta }\\n\\n. If the gradient of \\n\\n\\n\\nρ\\n\\n\\n{\\\\displaystyle \\\\rho }\\n\\n was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21] (which is known as the likelihood ratio method in the simulation-based optimization literature).[22]A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[23]Policy search methods have been used in the robotics context.[24] Many policy search methods may get stuck in local optima (as they are based on local search).\", 'Model-based algorithms': \"Finally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26] to the learning algorithm.Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]There are other ways to use models than to update a value function.[28] For instance, in model predictive control the model is used to update the behavior directly.\", 'Theory': 'Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.Efficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).', 'Research': 'Research topics include:', 'Comparison of key algorithms': '', 'Associative reinforcement learning': 'Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[46]', 'Deep reinforcement learning': 'This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[47] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[48]', 'Adversarial deep reinforcement learning': 'Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[49][50][51] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[52]', 'Fuzzy reinforcement learning': 'By introducing fuzzy inference in reinforcement learning,[53] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [54] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).', 'Inverse reinforcement learning': 'In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[55] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [56] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [57] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.', 'Safe reinforcement learning': 'Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[58] An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the Conditional Value at Risk (CVaR).[59] In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.[60][61] However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias[62] and blindness to success.[63]', 'Statistical comparison of reinforcement learning algorithms': 'Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[64] After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test.[65] This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[66]', 'See also': '', 'References': '', 'Sources': '', 'Further reading': '', 'External links': ''}\n",
            "**********************************************************************************************\n",
            "['#bodyContent', '/wiki/Main_Page', '/wiki/Wikipedia:Contents', '/wiki/Portal:Current_events', '/wiki/Special:Random', '/wiki/Wikipedia:About', '//en.wikipedia.org/wiki/Wikipedia:Contact_us', 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en', '/wiki/Help:Contents', '/wiki/Help:Introduction', '/wiki/Wikipedia:Community_portal', '/wiki/Special:RecentChanges', '/wiki/Wikipedia:File_upload_wizard', '/wiki/Main_Page', '/wiki/Special:Search', '/w/index.php?title=Special:CreateAccount&returnto=Reinforcement+learning', '/w/index.php?title=Special:UserLogin&returnto=Reinforcement+learning', '/w/index.php?title=Special:CreateAccount&returnto=Reinforcement+learning', '/w/index.php?title=Special:UserLogin&returnto=Reinforcement+learning', '/wiki/Help:Introduction', '/wiki/Special:MyContributions', '/wiki/Special:MyTalk', '#', '#Introduction', '#Exploration', '#Algorithms_for_control_learning', '#Criterion_of_optimality', '#Policy', '#State-value_function', '#Brute_force', '#Value_function', '#Monte_Carlo_methods', '#Temporal_difference_methods', '#Function_approximation_methods', '#Direct_policy_search', '#Model-based_algorithms', '#Theory', '#Research', '#Comparison_of_key_algorithms', '#Associative_reinforcement_learning', '#Deep_reinforcement_learning', '#Adversarial_deep_reinforcement_learning', '#Fuzzy_reinforcement_learning', '#Inverse_reinforcement_learning', '#Safe_reinforcement_learning', '#Statistical_comparison_of_reinforcement_learning_algorithms', '#See_also', '#References', '#Sources', '#Further_reading', '#External_links', 'https://ar.wikipedia.org/wiki/%D8%AA%D8%B9%D9%84%D9%85_%D8%A8%D8%A7%D9%84%D8%AA%D8%B9%D8%B2%D9%8A%D8%B2', 'https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%B2%E0%A6%AC%E0%A6%B0%E0%A7%8D%E0%A6%A7%E0%A6%A8%E0%A6%AE%E0%A7%82%E0%A6%B2%E0%A6%95_%E0%A6%B6%E0%A6%BF%E0%A6%96%E0%A6%A8', 'https://bg.wikipedia.org/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D1%83%D1%82%D0%B2%D1%8A%D1%80%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5', 'https://bs.wikipedia.org/wiki/Podr%C5%BEano_u%C4%8Denje', 'https://ca.wikipedia.org/wiki/Aprenentatge_per_refor%C3%A7', 'https://cs.wikipedia.org/wiki/Zp%C4%9Btnovazebn%C3%AD_u%C4%8Den%C3%AD', 'https://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen', 'https://et.wikipedia.org/wiki/Stiimul%C3%B5pe', 'https://el.wikipedia.org/wiki/%CE%95%CE%BD%CE%B9%CF%83%CF%87%CF%85%CF%84%CE%B9%CE%BA%CE%AE_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7', 'https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo', 'https://fa.wikipedia.org/wiki/%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D8%AA%D9%82%D9%88%DB%8C%D8%AA%DB%8C', 'https://fr.wikipedia.org/wiki/Apprentissage_par_renforcement', 'https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5', 'https://hy.wikipedia.org/wiki/%D4%B1%D5%B4%D6%80%D5%A1%D5%BA%D5%B6%D5%A4%D5%B4%D5%A1%D5%B6_%D5%B8%D6%82%D5%BD%D5%B8%D6%82%D6%81%D5%B8%D6%82%D5%B4', 'https://id.wikipedia.org/wiki/Reinforcement_learning_(pemelajaran_mesin)', 'https://it.wikipedia.org/wiki/Apprendimento_per_rinforzo', 'https://he.wikipedia.org/wiki/%D7%9C%D7%9E%D7%99%D7%93%D7%AA_%D7%97%D7%99%D7%96%D7%95%D7%A7', 'https://ms.wikipedia.org/wiki/Pembelajaran_pengukuhan', 'https://nl.wikipedia.org/wiki/Reinforcement_learning', 'https://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92', 'https://no.wikipedia.org/wiki/Forsterkende_l%C3%A6ring', 'https://or.wikipedia.org/wiki/%E0%AC%B0%E0%AC%BF%E0%AC%8F%E0%AC%A8%E0%AD%8D%E0%AC%AB%E0%AD%8B%E0%AC%B0%E0%AD%8D%E0%AC%B8%E0%AC%AE%E0%AD%87%E0%AC%A3%E0%AD%8D%E0%AC%9F_%E0%AC%B2%E0%AC%B0%E0%AD%8D%E0%AC%A3%E0%AD%8D%E0%AC%A3%E0%AC%BF%E0%AC%99%E0%AD%8D%E0%AC%97%E0%AD%8D', 'https://pl.wikipedia.org/wiki/Uczenie_przez_wzmacnianie', 'https://qu.wikipedia.org/wiki/Kallpanchana_yachay', 'https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D0%BF%D0%BE%D0%B4%D0%BA%D1%80%D0%B5%D0%BF%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC', 'https://simple.wikipedia.org/wiki/Reinforcement_learning', 'https://sl.wikipedia.org/wiki/Spodbujevano_u%C4%8Denje', 'https://ckb.wikipedia.org/wiki/%D9%81%DB%8E%D8%B1%D8%A8%D9%88%D9%88%D9%86%DB%8C_%D8%A8%DB%95%DA%BE%DB%8E%D8%B2%DA%A9%D8%B1%D8%AF%D9%86%DB%95%D9%88%DB%95', 'https://sr.wikipedia.org/wiki/Podr%C5%BEano_u%C4%8Denje', 'https://fi.wikipedia.org/wiki/Vahvistusoppiminen', 'https://sv.wikipedia.org/wiki/F%C3%B6rst%C3%A4rkningsinl%C3%A4rning', 'https://tr.wikipedia.org/wiki/Peki%C5%9Ftirmeli_%C3%B6%C4%9Frenme', 'https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F_%D0%B7_%D0%BF%D1%96%D0%B4%D0%BA%D1%80%D1%96%D0%BF%D0%BB%D0%B5%D0%BD%D0%BD%D1%8F%D0%BC', 'https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_t%C4%83ng_c%C6%B0%E1%BB%9Dng', 'https://wuu.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0', 'https://zh-yue.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%B8%E7%BF%92', 'https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0', 'https://www.wikidata.org/wiki/Special:EntityPage/Q830687#sitelinks-wikipedia', '/wiki/Reinforcement_learning', '/wiki/Talk:Reinforcement_learning', '/wiki/Reinforcement_learning', '/w/index.php?title=Reinforcement_learning&action=edit', '/w/index.php?title=Reinforcement_learning&action=history', '/wiki/Reinforcement_learning', '/w/index.php?title=Reinforcement_learning&action=edit', '/w/index.php?title=Reinforcement_learning&action=history', '/wiki/Special:WhatLinksHere/Reinforcement_learning', '/wiki/Special:RecentChangesLinked/Reinforcement_learning', '/wiki/Wikipedia:File_Upload_Wizard', '/wiki/Special:SpecialPages', '/w/index.php?title=Reinforcement_learning&oldid=1246080001', '/w/index.php?title=Reinforcement_learning&action=info', '/w/index.php?title=Special:CiteThisPage&page=Reinforcement_learning&id=1246080001&wpFormIdentifier=titleform', '/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FReinforcement_learning', '/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FReinforcement_learning', 'https://www.wikidata.org/wiki/Special:EntityPage/Q830687', '/w/index.php?title=Special:DownloadAsPdf&page=Reinforcement_learning&action=show-download-screen', '/w/index.php?title=Reinforcement_learning&printable=yes', 'https://commons.wikimedia.org/wiki/Category:Reinforcement_learning', '/wiki/Reinforcement', '/wiki/Operant_conditioning', '/wiki/Machine_learning', '/wiki/Data_mining', '/wiki/Supervised_learning', '/wiki/Unsupervised_learning', '/wiki/Semi-supervised_learning', '/wiki/Self-supervised_learning', '/wiki/Meta-learning_(computer_science)', '/wiki/Online_machine_learning', '/wiki/Batch_learning', '/wiki/Curriculum_learning', '/wiki/Rule-based_machine_learning', '/wiki/Neuro-symbolic_AI', '/wiki/Neuromorphic_engineering', '/wiki/Quantum_machine_learning', '/wiki/Statistical_classification', '/wiki/Generative_model', '/wiki/Regression_analysis', '/wiki/Cluster_analysis', '/wiki/Dimensionality_reduction', '/wiki/Density_estimation', '/wiki/Anomaly_detection', '/wiki/Data_cleaning', '/wiki/Automated_machine_learning', '/wiki/Association_rule_learning', '/wiki/Semantic_analysis_(machine_learning)', '/wiki/Structured_prediction', '/wiki/Feature_engineering', '/wiki/Feature_learning', '/wiki/Learning_to_rank', '/wiki/Grammar_induction', '/wiki/Ontology_learning', '/wiki/Multimodal_learning', '/wiki/Supervised_learning', '/wiki/Statistical_classification', '/wiki/Regression_analysis', '/wiki/Apprenticeship_learning', '/wiki/Decision_tree_learning', '/wiki/Ensemble_learning', '/wiki/Bootstrap_aggregating', '/wiki/Boosting_(machine_learning)', '/wiki/Random_forest', '/wiki/K-nearest_neighbors_algorithm', '/wiki/Linear_regression', '/wiki/Naive_Bayes_classifier', '/wiki/Artificial_neural_network', '/wiki/Logistic_regression', '/wiki/Perceptron', '/wiki/Relevance_vector_machine', '/wiki/Support_vector_machine', '/wiki/Cluster_analysis', '/wiki/BIRCH', '/wiki/CURE_algorithm', '/wiki/Hierarchical_clustering', '/wiki/K-means_clustering', '/wiki/Fuzzy_clustering', '/wiki/Expectation%E2%80%93maximization_algorithm', '/wiki/DBSCAN', '/wiki/OPTICS_algorithm', '/wiki/Mean_shift', '/wiki/Dimensionality_reduction', '/wiki/Factor_analysis', '/wiki/Canonical_correlation', '/wiki/Independent_component_analysis', '/wiki/Linear_discriminant_analysis', '/wiki/Non-negative_matrix_factorization', '/wiki/Principal_component_analysis', '/wiki/Proper_generalized_decomposition', '/wiki/T-distributed_stochastic_neighbor_embedding', '/wiki/Sparse_dictionary_learning', '/wiki/Structured_prediction', '/wiki/Graphical_model', '/wiki/Bayesian_network', '/wiki/Conditional_random_field', '/wiki/Hidden_Markov_model', '/wiki/Anomaly_detection', '/wiki/Random_sample_consensus', '/wiki/K-nearest_neighbors_algorithm', '/wiki/Local_outlier_factor', '/wiki/Isolation_forest', '/wiki/Artificial_neural_network', '/wiki/Autoencoder', '/wiki/Deep_learning', '/wiki/Feedforward_neural_network', '/wiki/Recurrent_neural_network', '/wiki/Long_short-term_memory', '/wiki/Gated_recurrent_unit', '/wiki/Echo_state_network', '/wiki/Reservoir_computing', '/wiki/Boltzmann_machine', '/wiki/Restricted_Boltzmann_machine', '/wiki/Generative_adversarial_network', '/wiki/Diffusion_model', '/wiki/Self-organizing_map', '/wiki/Convolutional_neural_network', '/wiki/U-Net', '/wiki/LeNet', '/wiki/AlexNet', '/wiki/DeepDream', '/wiki/Neural_radiance_field', '/wiki/Transformer_(machine_learning_model)', '/wiki/Vision_transformer', '/wiki/Mamba_(deep_learning_architecture)', '/wiki/Spiking_neural_network', '/wiki/Memtransistor', '/wiki/Electrochemical_RAM', '/wiki/Q-learning', '/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action', '/wiki/Temporal_difference_learning', '/wiki/Multi-agent_reinforcement_learning', '/wiki/Self-play_(reinforcement_learning_technique)', '/wiki/Active_learning_(machine_learning)', '/wiki/Crowdsourcing', '/wiki/Human-in-the-loop', '/wiki/Reinforcement_learning_from_human_feedback', '/wiki/Coefficient_of_determination', '/wiki/Confusion_matrix', '/wiki/Learning_curve_(machine_learning)', '/wiki/Receiver_operating_characteristic', '/wiki/Kernel_machines', '/wiki/Bias%E2%80%93variance_tradeoff', '/wiki/Computational_learning_theory', '/wiki/Empirical_risk_minimization', '/wiki/Occam_learning', '/wiki/Probably_approximately_correct_learning', '/wiki/Statistical_learning_theory', '/wiki/Vapnik%E2%80%93Chervonenkis_theory', '/wiki/ECML_PKDD', '/wiki/Conference_on_Neural_Information_Processing_Systems', '/wiki/International_Conference_on_Machine_Learning', '/wiki/International_Conference_on_Learning_Representations', '/wiki/International_Joint_Conference_on_Artificial_Intelligence', '/wiki/Machine_Learning_(journal)', '/wiki/Journal_of_Machine_Learning_Research', '/wiki/Glossary_of_artificial_intelligence', '/wiki/List_of_datasets_for_machine-learning_research', '/wiki/List_of_datasets_in_computer_vision_and_image_processing', '/wiki/Outline_of_machine_learning', '/wiki/Template:Machine_learning', '/wiki/Template_talk:Machine_learning', '/wiki/Special:EditPage/Template:Machine_learning', '/wiki/Machine_learning', '/wiki/Optimal_control', '/wiki/Intelligent_agent', '/wiki/Action_selection', '/wiki/Reward-based_selection', '/wiki/Machine_learning#Approaches', '/wiki/Supervised_learning', '/wiki/Unsupervised_learning', '/wiki/Q-learning', '#cite_note-kaelbling-1', '/wiki/Markov_decision_process', '/wiki/Dynamic_programming', '#cite_note-2', '#cite_note-Li-2023-3', '/w/index.php?title=Reinforcement_learning&action=edit&section=1', '/wiki/File:Reinforcement_learning_diagram.svg', '/wiki/Game_theory', '/wiki/Control_theory', '/wiki/Operations_research', '/wiki/Information_theory', '/wiki/Simulation-based_optimization', '/wiki/Multi-agent_system', '/wiki/Swarm_intelligence', '/wiki/Statistics', '/wiki/Optimal_control_theory', '/wiki/Markov_decision_process', '/wiki/Reinforcement', '#cite_note-4', '#cite_note-5', '/wiki/Partially_observable_Markov_decision_process', '/wiki/Regret_(game_theory)', '#cite_note-6', '#cite_note-7', '#cite_note-8', '/wiki/Backgammon', '/wiki/Checkers', '#cite_note-FOOTNOTESuttonBarto2018Chapter_11-9', '/wiki/Go_(game)', '/wiki/AlphaGo', '/wiki/Self-driving_car', '#cite_note-Ren-2022-10', '/wiki/Closed-form_expression', '/wiki/Simulation-based_optimization', '#cite_note-11', '/wiki/Machine_learning', '/w/index.php?title=Reinforcement_learning&action=edit&section=2', '/wiki/Multi-armed_bandit', '#cite_note-Optimal_adaptive_policies_for_Marko-12', '#cite_note-13', '/w/index.php?title=Reinforcement_learning&action=edit&section=3', '/w/index.php?title=Reinforcement_learning&action=edit&section=4', '/w/index.php?title=Reinforcement_learning&action=edit&section=5', '#cite_note-:0-14', '/w/index.php?title=Reinforcement_learning&action=edit&section=6', '#cite_note-:0-14', '/wiki/Q-learning#Discount_factor', '/w/index.php?title=Reinforcement_learning&action=edit&section=7', '/wiki/Brute-force_search', '#Value_function', '#Direct_policy_search', '/w/index.php?title=Reinforcement_learning&action=edit&section=8', '/wiki/Value_function', '/wiki/Value_iteration', '/wiki/Policy_iteration', '/w/index.php?title=Reinforcement_learning&action=edit&section=9', '/wiki/Monte_Carlo_sampling', '#cite_note-15', '/wiki/Simulation', '/wiki/Markov_chain', '/wiki/Dynamic_programming', '/wiki/Random_sampling', '/wiki/Multi-armed_bandit', '/wiki/Non-stationary', '/wiki/Value_function', '/wiki/Markov_decision_process', '/wiki/Mathematical_optimization', '#cite_note-:0-14', '/w/index.php?title=Reinforcement_learning&action=edit&section=10', '/wiki/Temporal_difference_learning', '/wiki/Temporal_difference', '/wiki/Bellman_equation', '#cite_note-16', '#cite_note-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]-17', '#cite_note-18', '/w/index.php?title=Reinforcement_learning&action=edit&section=11', '/wiki/Nonparametric_statistics', '/wiki/Q-learning', '#cite_note-19', '#cite_note-MBK-20', '/w/index.php?title=Reinforcement_learning&action=edit&section=12', '/wiki/Stochastic_optimization', '/wiki/Gradient', '/wiki/Gradient_descent', '#cite_note-21', '/wiki/Simulation-based_optimization', '#cite_note-22', '/wiki/Simulated_annealing', '/wiki/Cross-entropy_method', '/wiki/Evolutionary_computation', '#cite_note-23', '/wiki/Robotics', '#cite_note-24', '/wiki/Local_search_(optimization)', '/w/index.php?title=Reinforcement_learning&action=edit&section=13', '/wiki/Markov_decision_process', '#cite_note-25', '#cite_note-26', '#cite_note-27', '#cite_note-28', '/wiki/Model_predictive_control', '/w/index.php?title=Reinforcement_learning&action=edit&section=14', '#cite_note-Optimal_adaptive_policies_for_Marko-12', '/wiki/Wikipedia:Please_clarify', '/w/index.php?title=Reinforcement_learning&action=edit&section=15', '/wiki/File:Question_book-new.svg', '/wiki/Wikipedia:Verifiability', '/wiki/Special:EditPage/Reinforcement_learning', '/wiki/Help:Referencing_for_beginners', '/wiki/Help:Maintenance_template_removal', '#cite_note-29', '#cite_note-Li-2023-3', '#cite_note-30', '#cite_note-31', '/wiki/Reinforcement_learning_from_human_feedback', '#cite_note-32', '/wiki/Intrinsic_motivation_(artificial_intelligence)', '#cite_note-33', '#cite_note-34', '#cite_note-35', '#cite_note-36', '#cite_note-37', '/wiki/Partially_observable_Markov_decision_process', '/wiki/Predictive_state_representation', '#cite_note-kaplan2004-38', '#cite_note-klyubin2008-39', '#cite_note-barto2013-40', '/wiki/Monte_Carlo_tree_search', '#cite_note-41', '/wiki/Transfer_learning', '#cite_note-42', '/wiki/Dopamine', '/wiki/Dopaminergic', '/wiki/Substantia_nigra', '/wiki/Basal_ganglia', '/w/index.php?title=Reinforcement_learning&action=edit&section=16', '/wiki/Monte_Carlo_method', '/wiki/Temporal_difference_learning', '/wiki/Q-learning', '/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action', '/wiki/Q-learning#Deep_Q-learning', '/wiki/Proximal_Policy_Optimization', '/wiki/Distributional_Soft_Actor_Critic', '#cite_note-43', '#cite_note-44', '#cite_note-45', '/w/index.php?title=Reinforcement_learning&action=edit&section=17', '#cite_note-46', '/w/index.php?title=Reinforcement_learning&action=edit&section=18', '#cite_note-intro_deep_RL-47', '/wiki/DeepMind', '/wiki/Deep_reinforcement_learning', '/wiki/End-to-end_reinforcement_learning', '#cite_note-DQN2-48', '/w/index.php?title=Reinforcement_learning&action=edit&section=19', '#cite_note-49', '#cite_note-50', '#cite_note-51', '#cite_note-52', '/w/index.php?title=Reinforcement_learning&action=edit&section=20', '/wiki/Fuzzy_control_system', '#cite_note-53', '/wiki/Fuzzy_rule', '#cite_note-54', '/w/index.php?title=Reinforcement_learning&action=edit&section=21', '#cite_note-55', '#cite_note-56', '#cite_note-57', '/wiki/Random_utility_model', '/w/index.php?title=Reinforcement_learning&action=edit&section=22', '#cite_note-58', '/wiki/Expected_shortfall', '#cite_note-59', '#cite_note-60', '#cite_note-61', '#cite_note-62', '#cite_note-63', '/w/index.php?title=Reinforcement_learning&action=edit&section=23', '#cite_note-64', '/wiki/I.i.d', '/wiki/Student%27s_t-test', '/wiki/Permutation_test', '#cite_note-65', '#cite_note-66', '/w/index.php?title=Reinforcement_learning&action=edit&section=24', '/wiki/Temporal_difference_learning', '/wiki/Q-learning', '/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action', '/wiki/Reinforcement_learning_from_human_feedback', '/wiki/Optimal_control', '/wiki/Error-driven_learning', '/wiki/Multi-agent_reinforcement_learning', '/wiki/Apprenticeship_learning', '/wiki/Model-free_(reinforcement_learning)', '/w/index.php?title=Model-based_reinforcement_learning&action=edit&redlink=1', '/w/index.php?title=Direct_reinforcement_learning&action=edit&redlink=1', '#cite_note-Li-2023-3', '#cite_note-Guan-2021-67', '/w/index.php?title=Indirect_reinforcement_learning&action=edit&redlink=1', '#cite_note-Li-2023-3', '#cite_note-Guan-2021-67', '/wiki/Active_learning_(machine_learning)', '/w/index.php?title=Reinforcement_learning&action=edit&section=25', '#cite_ref-kaelbling_1-0', '/wiki/Leslie_P._Kaelbling', '/wiki/Michael_L._Littman', '/w/index.php?title=Andrew_W._Moore&action=edit&redlink=1', 'http://webarchive.loc.gov/all/20011120234539/http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/cs/9605103', '/wiki/Doi_(identifier)', 'https://doi.org/10.1613%2Fjair.301', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:1708582', 'http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html', '#cite_ref-2', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2F978-3-642-27645-3_1', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-3-642-27644-6', '#cite_ref-Li-2023_3-0', '#cite_ref-Li-2023_3-1', '#cite_ref-Li-2023_3-2', '#cite_ref-Li-2023_3-3', 'https://link.springer.com/book/10.1007/978-981-19-7784-8', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2F978-981-19-7784-8', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-9-811-97783-1', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:257928563', '/wiki/Template:Cite_book', '/wiki/Category:CS1_maint:_location_missing_publisher', '#cite_ref-4', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-0-13-604259-4', '/wiki/Template:Cite_book', '/wiki/Category:CS1_maint:_location_missing_publisher', '#cite_ref-5', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621', '/wiki/Doi_(identifier)', 'https://doi.org/10.1146%2Fannurev-neuro-062111-150512', '/wiki/PMC_(identifier)', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621', '/wiki/PMID_(identifier)', 'https://pubmed.ncbi.nlm.nih.gov/22462543', '#cite_ref-6', 'https://doi.org/10.1016%2Fj.epsr.2022.108515', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2022EPSR..21208515S', '/wiki/Doi_(identifier)', 'https://doi.org/10.1016%2Fj.epsr.2022.108515', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:250635151', '#cite_ref-7', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2005.04323', 'https://arxiv.org/archive/cs.GR', '#cite_ref-8', 'https://doi.org/10.1016%2Fj.ijepes.2021.107628', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2022IJEPE.13607628V', '/wiki/Doi_(identifier)', 'https://doi.org/10.1016%2Fj.ijepes.2021.107628', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:244099841', '#cite_ref-FOOTNOTESuttonBarto2018Chapter_11_9-0', '#CITEREFSuttonBarto2018', '#cite_ref-Ren-2022_10-0', 'https://ieeexplore.ieee.org/document/9857655', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2110.12359', '/wiki/Doi_(identifier)', 'https://doi.org/10.1109%2FTITS.2022.3196167', '#cite_ref-11', '/w/index.php?title=Abhijit_Gosavi&action=edit&redlink=1', 'https://www.springer.com/mathematics/applications/book/978-1-4020-7454-7', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-4020-7454-7', '#cite_ref-Optimal_adaptive_policies_for_Marko_12-0', '#cite_ref-Optimal_adaptive_policies_for_Marko_12-1', '/wiki/Michael_N._Katehakis', '/wiki/Mathematics_of_Operations_Research', '/wiki/Doi_(identifier)', 'https://doi.org/10.1287%2Fmoor.22.1.222', '/wiki/JSTOR_(identifier)', 'https://www.jstor.org/stable/3690147', '#cite_ref-13', 'http://www.tokic.com/www/tokicm/publikationen/papers/KI2011.pdf', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-3-642-24455-1', '#cite_ref-:0_14-0', '#cite_ref-:0_14-1', '#cite_ref-:0_14-2', 'https://web.archive.org/web/20170712170739/http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf', 'http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf', '#cite_ref-15', 'https://link.springer.com/article/10.1007/BF00114726', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2FBF00114726', '/wiki/ISSN_(identifier)', 'https://search.worldcat.org/issn/1573-0565', '#cite_ref-16', '/wiki/Richard_S._Sutton', 'https://web.archive.org/web/20170330002227/http://incompleteideas.net/sutton/publications.html#PhDthesis', 'http://incompleteideas.net/sutton/publications.html#PhDthesis', '#cite_ref-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]_17-0', '#CITEREFSuttonBarto2018', 'http://incompleteideas.net/sutton/book/ebook/node60.html', '#cite_ref-18', '/w/index.php?title=Steven_J._Bradtke&action=edit&redlink=1', '/wiki/Andrew_G._Barto', '/wiki/CiteSeerX_(identifier)', 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.143.857', '/wiki/Doi_(identifier)', 'https://doi.org/10.1023%2FA%3A1018056104778', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:20327856', '#cite_ref-19', '/w/index.php?title=Christopher_J.C.H._Watkins&action=edit&redlink=1', 'http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf', '#cite_ref-MBK_20-0', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9407070', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2022Entrp..24.1168M', '/wiki/Doi_(identifier)', 'https://doi.org/10.3390%2Fe24081168', '/wiki/PMC_(identifier)', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9407070', '/wiki/PMID_(identifier)', 'https://pubmed.ncbi.nlm.nih.gov/36010832', '#cite_ref-21', '/wiki/Ronald_J._Williams', '/wiki/CiteSeerX_(identifier)', 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.8871', '#cite_ref-22', '/wiki/Jan_Peters_(computer_scientist)', '/wiki/Sethu_Vijayakumar', '/wiki/Stefan_Schaal', 'http://web.archive.org/web/20130512223911/http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf', 'http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf', '#cite_ref-23', 'https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2', '#cite_ref-24', '/w/index.php?title=Marc_Peter_Deisenroth&action=edit&redlink=1', '/wiki/Gerhard_Neumann', '/wiki/Jan_Peters_(computer_scientist)', 'http://eprints.lincoln.ac.uk/28029/1/PolicySearchReview.pdf', '/wiki/Doi_(identifier)', 'https://doi.org/10.1561%2F2300000021', '/wiki/Hdl_(identifier)', 'https://hdl.handle.net/10044%2F1%2F12051', '#cite_ref-25', '#cite_ref-26', 'https://link.springer.com/content/pdf/10.1007/BF00992699.pdf', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2FBF00992699', '#cite_ref-27', 'https://www.sciencedirect.com/science/article/pii/B9780323899314000110', '/wiki/Doi_(identifier)', 'https://doi.org/10.1016%2Fb978-0-323-89931-4.00011-0', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-0-323-89931-4', '#cite_ref-28', 'https://proceedings.neurips.cc/paper/2019/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf', '#cite_ref-29', 'https://dl.acm.org/doi/10.1109/TSMCB.2011.2170565', '/wiki/Doi_(identifier)', 'https://doi.org/10.1109%2FTSMCB.2011.2170565', '/wiki/ISSN_(identifier)', 'https://search.worldcat.org/issn/1083-4419', '#cite_ref-30', 'https://cie.acm.org/articles/use-reinforcements-learning-testing-game-mechanics/', '#cite_ref-31', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2Fs10458-019-09404-2', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:71147890', '#cite_ref-32', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2111.08596', 'https://arxiv.org/archive/cs.LG', '#cite_ref-33', 'http://dl.acm.org/citation.cfm?id=3157382.3157509', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1604.06057', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2016arXiv160406057K', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-5108-3881-9', '#cite_ref-34', 'http://umichrl.pbworks.com/Successes-of-Reinforcement-Learning/', '#cite_ref-35', 'https://ieeexplore.ieee.org/document/9116294', 'http://repository.essex.ac.uk/27546/1/User%20Interaction%20Aware%20Reinforcement%20Learning.pdf', '/wiki/Doi_(identifier)', 'https://doi.org/10.23919%2FDATE48585.2020.9116294', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-3-9819263-4-7', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:219858480', '#cite_ref-36', 'https://www.businessweekly.co.uk/news/academia-research/smartphones-get-smarter-essex-innovation', '#cite_ref-37', 'https://inews.co.uk/news/technology/future-smartphones-prolong-battery-life-monitoring-behaviour-558689', '/wiki/I_(newspaper)', '#cite_ref-kaplan2004_38-0', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2F978-3-540-27833-7_19', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-3-540-22484-6', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:9781221', '#cite_ref-klyubin2008_39-0', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607028', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2008PLoSO...3.4018K', '/wiki/Doi_(identifier)', 'https://doi.org/10.1371%2Fjournal.pone.0004018', '/wiki/PMC_(identifier)', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607028', '/wiki/PMID_(identifier)', 'https://pubmed.ncbi.nlm.nih.gov/19107219', '#cite_ref-barto2013_40-0', 'https://people.cs.umass.edu/~barto/IMCleVer-chapter-totypeset2.pdf', '#cite_ref-41', '/wiki/SSRN_(identifier)', 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3374766', '#cite_ref-42', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1811.08318', '/wiki/Doi_(identifier)', 'https://doi.org/10.1177%2F1059712318818568', '/wiki/ISSN_(identifier)', 'https://search.worldcat.org/issn/1059-7123', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:53774629', '#cite_ref-43', 'https://ieeexplore.ieee.org/document/9448360', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2001.02811', '/wiki/Doi_(identifier)', 'https://doi.org/10.1109%2FTNNLS.2021.3082568', '/wiki/PMID_(identifier)', 'https://pubmed.ncbi.nlm.nih.gov/34101599', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:211259373', '#cite_ref-44', 'https://ieeexplore.ieee.org/document/9294300', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2002.05502', '/wiki/Doi_(identifier)', 'https://doi.org/10.1109%2FITSC45102.2020.9294300', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-7281-4149-7', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:211096594', '#cite_ref-45', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2310.05858', 'https://arxiv.org/archive/cs.LG', '#cite_ref-46', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/0-471-55717-X', '#cite_ref-intro_deep_RL_47-0', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1811.12560', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2018arXiv181112560F', '/wiki/Doi_(identifier)', 'https://doi.org/10.1561%2F2200000071', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:54434537', '#cite_ref-DQN2_48-0', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2015Natur.518..529M', '/wiki/Doi_(identifier)', 'https://doi.org/10.1038%2Fnature14236', '/wiki/PMID_(identifier)', 'https://pubmed.ncbi.nlm.nih.gov/25719670', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:205242740', '#cite_ref-49', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1412.6572', '#cite_ref-50', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1701.04143', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2F978-3-319-62416-7_19', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-3-319-62415-0', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:1562290', '#cite_ref-51', 'http://worldcat.org/oclc/1106256905', '/wiki/OCLC_(identifier)', 'https://search.worldcat.org/oclc/1106256905', '/wiki/Template:Cite_book', '/wiki/Category:CS1_maint:_multiple_names:_authors_list', '#cite_ref-52', 'https://doi.org/10.1609%2Faaai.v36i7.20684', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2112.09025', '/wiki/Doi_(identifier)', 'https://doi.org/10.1609%2Faaai.v36i7.20684', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:245219157', '#cite_ref-53', 'https://ieeexplore.ieee.org/document/343737', '/wiki/Doi_(identifier)', 'https://doi.org/10.1109%2FFUZZY.1994.343737', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/0-7803-1896-X', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:56694947', '#cite_ref-54', 'http://users.iit.uni-miskolc.hu/~vinczed/research/vinczed_sami2017_author_draft.pdf', '/wiki/Doi_(identifier)', 'https://doi.org/10.1109%2FSAMI.2017.7880298', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-5090-5655-2', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:17590120', '#cite_ref-55', 'https://ai.stanford.edu/~ang/papers/icml00-irl.pdf', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/1-55860-707-2', '#cite_ref-56', 'https://dl.acm.org/doi/10.5555/1620270.1620297', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-57735-368-3', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:336219', '#cite_ref-57', 'https://doi.org/10.1016/j.ins.2024.120128', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2105.12092', '/wiki/Doi_(identifier)', 'https://doi.org/10.1016%2Fj.ins.2024.120128', '/wiki/ISSN_(identifier)', 'https://search.worldcat.org/issn/0020-0255', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:235187141', '#cite_ref-58', 'https://jmlr.org/papers/volume16/garcia15a/garcia15a.pdf', '#cite_ref-59', 'https://proceedings.mlr.press/v80/dabney18a.html', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1806.06923', '#cite_ref-60', 'https://proceedings.neurips.cc/paper/2015/hash/64223ccf70bbb65a3a4aceac37e21016-Abstract.html', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1506.02188', '#cite_ref-61', 'https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LnwyFkkAAAAJ&citation_for_view=LnwyFkkAAAAJ:eQOLeE2rZwMC', '#cite_ref-62', 'https://ojs.aaai.org/index.php/AAAI/article/view/9561', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1404.3862', '/wiki/Doi_(identifier)', 'https://doi.org/10.1609%2Faaai.v29i1.9561', '/wiki/ISSN_(identifier)', 'https://search.worldcat.org/issn/2374-3468', '#cite_ref-63', 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/d2511dfb731fa336739782ba825cd98c-Abstract-Conference.html', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2205.05138', '#cite_ref-64', 'https://openreview.net/forum?id=r1etN1rtPB', '#cite_ref-65', 'https://openreview.net/forum?id=ryx0N3IaIV', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1904.06979', '#cite_ref-66', 'https://proceedings.mlr.press/v139/greenberg21a.html', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/2010.11660', '#cite_ref-Guan-2021_67-0', '#cite_ref-Guan-2021_67-1', 'https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22466', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1912.10600', '/wiki/Doi_(identifier)', 'https://doi.org/10.1002%2Fint.22466', '/w/index.php?title=Reinforcement_learning&action=edit&section=26', '/wiki/Richard_S._Sutton', '/wiki/Andrew_Barto', 'http://incompleteideas.net/sutton/book/the-book.html', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-0-262-03924-6', 'https://link.springer.com/book/10.1007/978-981-19-7784-8', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2F978-981-19-7784-8', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-9-811-97783-1', 'http://www.mit.edu/~dimitrib/RLbook.html', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-886-52939-7', '/w/index.php?title=Reinforcement_learning&action=edit&section=27', 'https://doi.org/10.1146%2Fannurev-control-062922-090153', '/wiki/Doi_(identifier)', 'https://doi.org/10.1146%2Fannurev-control-062922-090153', '/wiki/ISSN_(identifier)', 'https://search.worldcat.org/issn/2573-5144', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:255702873', '/wiki/Peter_Auer', 'http://jmlr.csail.mit.edu/papers/v11/jaksch10a.html', '/wiki/Bart_De_Schutter', 'http://www.dcsc.tudelft.nl/rlbook/', '/wiki/ISBN_(identifier)', '/wiki/Special:BookSources/978-1-4398-2108-4', '/wiki/ArXiv_(identifier)', 'https://arxiv.org/abs/1811.12560', '/wiki/Bibcode_(identifier)', 'https://ui.adsabs.harvard.edu/abs/2018arXiv181112560F', '/wiki/Doi_(identifier)', 'https://doi.org/10.1561%2F2200000071', '/wiki/S2CID_(identifier)', 'https://api.semanticscholar.org/CorpusID:54434537', 'https://web.archive.org/web/20160731230325/http://castlelab.princeton.edu/adp.htm', 'http://www.castlelab.princeton.edu/adp.htm', '/wiki/Richard_S._Sutton', 'https://doi.org/10.1007%2FBF00115009', '/wiki/Doi_(identifier)', 'https://doi.org/10.1007%2FBF00115009', 'https://web.archive.org/web/20100714095438/http://www.icml2010.org/papers/546.pdf', 'http://www.icml2010.org/papers/546.pdf', '/w/index.php?title=Reinforcement_learning&action=edit&section=28', 'https://all.cs.umass.edu/rlr/', 'http://spaces.facsci.ualberta.ca/rlai/', '/wiki/University_of_Alberta', 'http://www-all.cs.umass.edu/', '/wiki/University_of_Massachusetts_Amherst', 'http://www.dcsc.tudelft.nl/~robotics/media.html', 'https://web.archive.org/web/20181008104644/http://www.dcsc.tudelft.nl/~robotics/media.html', '/wiki/Wayback_Machine', '/wiki/Delft_University_of_Technology', 'https://www.youtube.com/watch?v=RtxI449ZjSc&feature=relmfu', 'https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html', 'https://lilianweng.github.io/posts/2018-02-19-rl-overview/', '/wiki/Template:Differentiable_computing', '/wiki/Template_talk:Differentiable_computing', '/wiki/Special:EditPage/Template:Differentiable_computing', '/wiki/Differentiable_function', '/wiki/Differentiable_programming', '/wiki/Information_geometry', '/wiki/Statistical_manifold', '/wiki/Automatic_differentiation', '/wiki/Neuromorphic_engineering', '/wiki/Pattern_recognition', '/wiki/Tensor_calculus', '/wiki/Computational_learning_theory', '/wiki/Inductive_bias', '/wiki/Gradient_descent', '/wiki/Stochastic_gradient_descent', '/wiki/Cluster_analysis', '/wiki/Regression_analysis', '/wiki/Overfitting', '/wiki/Hallucination_(artificial_intelligence)', '/wiki/Adversarial_machine_learning', '/wiki/Attention_(machine_learning)', '/wiki/Convolution', '/wiki/Loss_functions_for_classification', '/wiki/Backpropagation', '/wiki/Batch_normalization', '/wiki/Activation_function', '/wiki/Softmax_function', '/wiki/Sigmoid_function', '/wiki/Rectifier_(neural_networks)', '/wiki/Regularization_(mathematics)', '/wiki/Training,_validation,_and_test_sets', '/wiki/Data_augmentation', '/wiki/Diffusion_process', '/wiki/Autoregressive_model', '/wiki/Machine_learning', '/wiki/Prompt_engineering#In-context_learning', '/wiki/Artificial_neural_network', '/wiki/Deep_learning', '/wiki/Computational_science', '/wiki/Artificial_intelligence', '/wiki/Language_model', '/wiki/Large_language_model', '/wiki/Graphcore', '/wiki/Tensor_Processing_Unit', '/wiki/Vision_processing_unit', '/wiki/Memristor', '/wiki/SpiNNaker', '/wiki/TensorFlow', '/wiki/PyTorch', '/wiki/Keras', '/wiki/Theano_(software)', '/wiki/Google_JAX', '/wiki/Flux_(machine-learning_framework)', '/wiki/MindSpore', '/wiki/AlexNet', '/wiki/WaveNet', '/wiki/Human_image_synthesis', '/wiki/Handwriting_recognition', '/wiki/Optical_character_recognition', '/wiki/Deep_learning_speech_synthesis', '/wiki/Speech_recognition', '/wiki/Facial_recognition_system', '/wiki/AlphaFold', '/wiki/Text-to-image_model', '/wiki/DALL-E', '/wiki/Midjourney', '/wiki/Stable_Diffusion', '/wiki/Text-to-video_model', '/wiki/Sora_(text-to-video_model)', '/wiki/VideoPoet', '/wiki/Whisper_(speech_recognition_system)', '/wiki/Word2vec', '/wiki/Seq2seq', '/wiki/BERT_(language_model)', '/wiki/Claude_(language_model)', '/wiki/Gemini_(language_model)', '/wiki/LaMDA', '/wiki/Bard_(chatbot)', '/wiki/Neural_machine_translation', '/wiki/Project_Debater', '/wiki/IBM_Watson', '/wiki/IBM_Watsonx', '/wiki/IBM_Granite', '/wiki/GPT-1', '/wiki/GPT-2', '/wiki/GPT-3', '/wiki/GPT-4', '/wiki/ChatGPT', '/wiki/GPT-J', '/wiki/Chinchilla_AI', '/wiki/PaLM', '/wiki/BLOOM_(language_model)', '/wiki/LLaMA', '/wiki/Huawei_PanGu', '/wiki/AlphaGo', '/wiki/AlphaZero', '/wiki/Q-learning', '/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action', '/wiki/OpenAI_Five', '/wiki/Self-driving_car', '/wiki/MuZero', '/wiki/Action_selection', '/wiki/Auto-GPT', '/wiki/Robot_control', '/wiki/Yoshua_Bengio', '/wiki/Alex_Graves_(computer_scientist)', '/wiki/Ian_Goodfellow', '/wiki/Stephen_Grossberg', '/wiki/Demis_Hassabis', '/wiki/Geoffrey_Hinton', '/wiki/Yann_LeCun', '/wiki/Fei-Fei_Li', '/wiki/Andrew_Ng', '/wiki/J%C3%BCrgen_Schmidhuber', '/wiki/David_Silver_(computer_scientist)', '/wiki/Ilya_Sutskever', '/wiki/Anthropic', '/wiki/EleutherAI', '/wiki/Google_DeepMind', '/wiki/Hugging_Face', '/wiki/OpenAI', '/wiki/Meta_AI', '/wiki/Mila_(research_institute)', '/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory', '/wiki/Huawei', '/wiki/Neural_Turing_machine', '/wiki/Differentiable_neural_computer', '/wiki/Transformer_(machine_learning_model)', '/wiki/Recurrent_neural_network', '/wiki/Long_short-term_memory', '/wiki/Gated_recurrent_unit', '/wiki/Echo_state_network', '/wiki/Multilayer_perceptron', '/wiki/Convolutional_neural_network', '/wiki/Residual_neural_network', '/wiki/Mamba_(deep_learning)', '/wiki/Autoencoder', '/wiki/Variational_autoencoder', '/wiki/Generative_adversarial_network', '/wiki/Graph_neural_network', '/wiki/File:Symbol_portal_class.svg', '/wiki/Portal:Computer_programming', '/wiki/Portal:Technology', '/wiki/Category:Artificial_neural_networks', '/wiki/Category:Machine_learning', '/wiki/Template:Computer_science', '/wiki/Template_talk:Computer_science', '/wiki/Special:EditPage/Template:Computer_science', '/wiki/Computer_science', '/wiki/ACM_Computing_Classification_System', '/wiki/Computer_hardware', '/wiki/Printed_circuit_board', '/wiki/Peripheral', '/wiki/Integrated_circuit', '/wiki/Very_Large_Scale_Integration', '/wiki/System_on_a_chip', '/wiki/Green_computing', '/wiki/Electronic_design_automation', '/wiki/Hardware_acceleration', '/wiki/Processor_(computing)', '/wiki/List_of_computer_size_categories', '/wiki/Form_factor_(design)', '/wiki/Computer_architecture', '/wiki/Computational_complexity', '/wiki/Dependability', '/wiki/Embedded_system', '/wiki/Real-time_computing', '/wiki/Computer_network', '/wiki/Network_architecture', '/wiki/Network_protocol', '/wiki/Networking_hardware', '/wiki/Network_scheduler', '/wiki/Network_performance', '/wiki/Network_service', '/wiki/Interpreter_(computing)', '/wiki/Middleware', '/wiki/Virtual_machine', '/wiki/Operating_system', '/wiki/Software_quality', '/wiki/Programming_language_theory', '/wiki/Programming_tool', '/wiki/Programming_paradigm', '/wiki/Programming_language', '/wiki/Compiler_construction', '/wiki/Domain-specific_language', '/wiki/Modeling_language', '/wiki/Software_framework', '/wiki/Integrated_development_environment', '/wiki/Software_configuration_management', '/wiki/Library_(computing)', '/wiki/Software_repository', '/wiki/Software_development', '/wiki/Control_variable_(programming)', '/wiki/Software_development_process', '/wiki/Requirements_analysis', '/wiki/Software_design', '/wiki/Software_construction', '/wiki/Software_deployment', '/wiki/Software_engineering', '/wiki/Software_maintenance', '/wiki/Programming_team', '/wiki/Open-source_software', '/wiki/Theory_of_computation', '/wiki/Model_of_computation', '/wiki/Formal_language', '/wiki/Automata_theory', '/wiki/Computability_theory', '/wiki/Computational_complexity_theory', '/wiki/Logic_in_computer_science', '/wiki/Semantics_(computer_science)', '/wiki/Algorithm', '/wiki/Algorithm_design', '/wiki/Analysis_of_algorithms', '/wiki/Algorithmic_efficiency', '/wiki/Randomized_algorithm', '/wiki/Computational_geometry', '/wiki/Computing', '/wiki/Discrete_mathematics', '/wiki/Probability', '/wiki/Statistics', '/wiki/Mathematical_software', '/wiki/Information_theory', '/wiki/Mathematical_analysis', '/wiki/Numerical_analysis', '/wiki/Theoretical_computer_science', '/wiki/Information_system', '/wiki/Database', '/wiki/Computer_data_storage', '/wiki/Enterprise_information_system', '/wiki/Social_software', '/wiki/Geographic_information_system', '/wiki/Decision_support_system', '/wiki/Process_control', '/wiki/Multimedia_database', '/wiki/Data_mining', '/wiki/Digital_library', '/wiki/Computing_platform', '/wiki/Digital_marketing', '/wiki/World_Wide_Web', '/wiki/Information_retrieval', '/wiki/Computer_security', '/wiki/Cryptography', '/wiki/Formal_methods', '/wiki/Security_hacker', '/wiki/Security_service_(telecommunication)', '/wiki/Intrusion_detection_system', '/wiki/Hardware_security', '/wiki/Network_security', '/wiki/Information_security', '/wiki/Application_security', '/wiki/Human%E2%80%93computer_interaction', '/wiki/Interaction_design', '/wiki/Social_computing', '/wiki/Ubiquitous_computing', '/wiki/Visualization_(graphics)', '/wiki/Computer_accessibility', '/wiki/Concurrency_(computer_science)', '/wiki/Concurrent_computing', '/wiki/Parallel_computing', '/wiki/Distributed_computing', '/wiki/Multithreading_(computer_architecture)', '/wiki/Multiprocessing', '/wiki/Artificial_intelligence', '/wiki/Natural_language_processing', '/wiki/Knowledge_representation_and_reasoning', '/wiki/Computer_vision', '/wiki/Automated_planning_and_scheduling', '/wiki/Mathematical_optimization', '/wiki/Control_theory', '/wiki/Philosophy_of_artificial_intelligence', '/wiki/Distributed_artificial_intelligence', '/wiki/Machine_learning', '/wiki/Supervised_learning', '/wiki/Unsupervised_learning', '/wiki/Multi-task_learning', '/wiki/Cross-validation_(statistics)', '/wiki/Computer_graphics', '/wiki/Computer_animation', '/wiki/Rendering_(computer_graphics)', '/wiki/Photograph_manipulation', '/wiki/Graphics_processing_unit', '/wiki/Mixed_reality', '/wiki/Virtual_reality', '/wiki/Image_compression', '/wiki/Solid_modeling', '/wiki/Quantum_Computing', '/wiki/E-commerce', '/wiki/Enterprise_software', '/wiki/Computational_mathematics', '/wiki/Computational_physics', '/wiki/Computational_chemistry', '/wiki/Computational_biology', '/wiki/Computational_social_science', '/wiki/Computational_engineering', '/wiki/Template:Differentiable_computing', '/wiki/Health_informatics', '/wiki/Digital_art', '/wiki/Electronic_publishing', '/wiki/Cyberwarfare', '/wiki/Electronic_voting', '/wiki/Video_game', '/wiki/Word_processor', '/wiki/Operations_research', '/wiki/Educational_technology', '/wiki/Document_management_system', '/wiki/Category:Computer_science', '/wiki/Outline_of_computer_science', '/wiki/Template:Glossaries_of_computers', 'https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&oldid=1246080001', '/wiki/Help:Category', '/wiki/Category:Reinforcement_learning', '/wiki/Category:Markov_models', '/wiki/Category:Belief_revision', '/wiki/Category:CS1_maint:_location_missing_publisher', '/wiki/Category:CS1_maint:_multiple_names:_authors_list', '/wiki/Category:Articles_with_short_description', '/wiki/Category:Short_description_matches_Wikidata', '/wiki/Category:Wikipedia_articles_needing_clarification_from_January_2020', '/wiki/Category:Articles_needing_additional_references_from_October_2022', '/wiki/Category:All_articles_needing_additional_references', '/wiki/Category:Webarchive_template_wayback_links', '//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License', '//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License', '//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use', '//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy', '//wikimediafoundation.org/', 'https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy', '/wiki/Wikipedia:About', '/wiki/Wikipedia:General_disclaimer', '//en.wikipedia.org/wiki/Wikipedia:Contact_us', 'https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct', 'https://developer.wikimedia.org', 'https://stats.wikimedia.org/#/en.wikipedia.org', 'https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement', '//en.m.wikipedia.org/w/index.php?title=Reinforcement_learning&mobileaction=toggle_view_mobile', 'https://wikimediafoundation.org/', 'https://www.mediawiki.org/']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJNUCGXXO043"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4T3rD0SjO01b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAk3dCNZO0yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# page = requests.get(\"https://en.wikipedia.org/wiki/Reinforcement_learning\")\n",
        "# soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "metadata": {
        "id": "afhnyOUmQMaC"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# soup.find(\"title\").get_text()"
      ],
      "metadata": {
        "id": "Rw3WSUtAQXLh"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# soup.find(\"a\", {\"class\":\"mw-jump-link\"})['href']\n",
        "\n",
        "# # \"https://en.wikipedia.org/wiki/Reinforcement_learning\"+\"#bodyContent\""
      ],
      "metadata": {
        "id": "5Qsgc1xoRy6M"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link = \"https://en.wikipedia.org/wiki/Reinforcement_learning\"\n",
        "\n",
        "# body = link + (soup.find(\"a\", {\"class\":\"mw-jump-link\"})[\"href\"])\n",
        "# body"
      ],
      "metadata": {
        "id": "lNWxkHbGSPLT"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pag = requests.get(\"https://en.wikipedia.org/wiki/Reinforcement_learning#bodyContent\")\n",
        "# soup1 = BeautifulSoup(pag.content, 'html.parser')\n",
        "\n",
        "# soup1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x3mRF7BKTaj1"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "div_list =  soup.find('div', class_ ='mw-heading mw-heading2' ).find_next_siblings()\n",
        "\n",
        "para_dict = {'Introduction': ''}\n",
        "\n",
        "key_name = 'Introduction'\n",
        "for each in div_list:\n",
        "  print(each.name)\n",
        "  if each.name == 'div':\n",
        "    print(each)\n",
        "    print(each.find('h2'))\n",
        "    ddiv = each.find([\"h2\",\"h3\", \"h4\"])\n",
        "    if ddiv != None:\n",
        "      key_name = ddiv.get_text()\n",
        "    if type(key_name) == str:\n",
        "      para_dict[key_name] = ''\n",
        "  elif each.name == 'p':\n",
        "    para_dict[key_name] = para_dict[key_name] + each.get_text().strip()\n",
        "    print(each.get_text().strip())\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s4EatskNpjEs",
        "outputId": "0fba6057-2822-4723-fc48-0b58e42d6818"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "figure\n",
            "p\n",
            "Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment.\n",
            "p\n",
            "Basic reinforcement learning is modeled as a Markov decision process:\n",
            "ul\n",
            "p\n",
            "The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the \"reward function\" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. (See Reinforcement.) For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]\n",
            "p\n",
            "A basic reinforcement learning AI agent interacts with its environment in discrete time steps. At each time t, the agent receives the current state \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle S_{t}}\n",
            "\n",
            " and reward \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "R\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle R_{t}}\n",
            "\n",
            ". It then chooses an action \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle A_{t}}\n",
            "\n",
            " from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "+\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle S_{t+1}}\n",
            "\n",
            " and the reward \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "R\n",
            "\n",
            "t\n",
            "+\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle R_{t+1}}\n",
            "\n",
            " associated with the transition \n",
            "\n",
            "\n",
            "\n",
            "(\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            ",\n",
            "\n",
            "A\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            ",\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "+\n",
            "1\n",
            "\n",
            "\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle (S_{t},A_{t},S_{t+1})}\n",
            "\n",
            " is determined. The goal of a reinforcement learning agent is to learn a policy: \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            ":\n",
            "\n",
            "\n",
            "S\n",
            "\n",
            "\n",
            "×\n",
            "\n",
            "\n",
            "A\n",
            "\n",
            "\n",
            "→\n",
            "[\n",
            "0\n",
            ",\n",
            "1\n",
            "]\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}\n",
            "\n",
            ", \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "(\n",
            "s\n",
            ",\n",
            "a\n",
            ")\n",
            "=\n",
            "Pr\n",
            "(\n",
            "\n",
            "A\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            "=\n",
            "a\n",
            "∣\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            "=\n",
            "s\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}\n",
            "\n",
            " that maximizes the expected cumulative reward.\n",
            "p\n",
            "Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\n",
            "p\n",
            "When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.\n",
            "p\n",
            "Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation,[6] robot control,[7] photovoltaic generators dispatch,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]\n",
            "p\n",
            "Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:\n",
            "ul\n",
            "p\n",
            "The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Exploration\">Exploration</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=2\" title=\"Edit section: Exploration\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Exploration\">Exploration</h2>\n",
            "p\n",
            "The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]\n",
            "p\n",
            "Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\n",
            "p\n",
            "One such method is \n",
            "\n",
            "\n",
            "\n",
            "ε\n",
            "\n",
            "\n",
            "{\\displaystyle \\varepsilon }\n",
            "\n",
            "-greedy, where \n",
            "\n",
            "\n",
            "\n",
            "0\n",
            "<\n",
            "ε\n",
            "<\n",
            "1\n",
            "\n",
            "\n",
            "{\\displaystyle 0<\\varepsilon <1}\n",
            "\n",
            " is a parameter controlling the amount of exploration vs. exploitation.  With probability \n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "−\n",
            "ε\n",
            "\n",
            "\n",
            "{\\displaystyle 1-\\varepsilon }\n",
            "\n",
            ", exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \n",
            "\n",
            "\n",
            "\n",
            "ε\n",
            "\n",
            "\n",
            "{\\displaystyle \\varepsilon }\n",
            "\n",
            ", exploration is chosen, and the action is chosen uniformly at random. \n",
            "\n",
            "\n",
            "\n",
            "ε\n",
            "\n",
            "\n",
            "{\\displaystyle \\varepsilon }\n",
            "\n",
            " is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Algorithms_for_control_learning\">Algorithms for control learning</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=3\" title=\"Edit section: Algorithms for control learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Algorithms_for_control_learning\">Algorithms for control learning</h2>\n",
            "p\n",
            "Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Criterion_of_optimality\">Criterion of optimality</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=4\" title=\"Edit section: Criterion of optimality\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "div\n",
            "<div class=\"mw-heading mw-heading4\"><h4 id=\"Policy\">Policy</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=5\" title=\"Edit section: Policy\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "The agent's action selection is modeled as a map called policy:\n",
            "dl\n",
            "p\n",
            "The policy map gives the probability of taking action \n",
            "\n",
            "\n",
            "\n",
            "a\n",
            "\n",
            "\n",
            "{\\displaystyle a}\n",
            "\n",
            " when in state \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            ".[14]: 61  There are also deterministic policies.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading4\"><h4 id=\"State-value_function\">State-value function</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=6\" title=\"Edit section: State-value function\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "The state-value function \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "V\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle V_{\\pi }(s)}\n",
            "\n",
            " is defined as, expected discounted return starting with state \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            ", i.e. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "S\n",
            "\n",
            "0\n",
            "\n",
            "\n",
            "=\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle S_{0}=s}\n",
            "\n",
            ", and successively following policy \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            ". Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.[14]: 60\n",
            "dl\n",
            "p\n",
            "where the random variable \n",
            "\n",
            "\n",
            "\n",
            "G\n",
            "\n",
            "\n",
            "{\\displaystyle G}\n",
            "\n",
            " denotes the discounted return, and is defined as the sum of future discounted rewards:\n",
            "dl\n",
            "p\n",
            "where \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "R\n",
            "\n",
            "t\n",
            "+\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle R_{t+1}}\n",
            "\n",
            " is the reward for transitioning from state \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle S_{t}}\n",
            "\n",
            " to \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "S\n",
            "\n",
            "t\n",
            "+\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle S_{t+1}}\n",
            "\n",
            ", \n",
            "\n",
            "\n",
            "\n",
            "0\n",
            "≤\n",
            "γ\n",
            "<\n",
            "1\n",
            "\n",
            "\n",
            "{\\displaystyle 0\\leq \\gamma <1}\n",
            "\n",
            " is the discount rate. \n",
            "\n",
            "\n",
            "\n",
            "γ\n",
            "\n",
            "\n",
            "{\\displaystyle \\gamma }\n",
            "\n",
            " is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\n",
            "p\n",
            "The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Brute_force\">Brute force</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=7\" title=\"Edit section: Brute force\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "The brute force approach entails two steps:\n",
            "ul\n",
            "p\n",
            "One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\n",
            "p\n",
            "These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Value_function\">Value function</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=8\" title=\"Edit section: Value function\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "link\n",
            "div\n",
            "<div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a href=\"/wiki/Value_function\" title=\"Value function\">Value function</a></div>\n",
            "None\n",
            "p\n",
            "Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "E\n",
            "\n",
            "\n",
            "⁡\n",
            "[\n",
            "G\n",
            "]\n",
            "\n",
            "\n",
            "{\\displaystyle \\operatorname {\\mathbb {E} } [G]}\n",
            "\n",
            " for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\n",
            "p\n",
            "These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\n",
            "p\n",
            "To define optimality in a formal manner, define the state-value of a policy \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            " by\n",
            "dl\n",
            "p\n",
            "where \n",
            "\n",
            "\n",
            "\n",
            "G\n",
            "\n",
            "\n",
            "{\\displaystyle G}\n",
            "\n",
            " stands for the discounted return associated with following \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            " from the initial state \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            ". Defining \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "V\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle V^{*}(s)}\n",
            "\n",
            " as the maximum possible state-value of \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "V\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle V^{\\pi }(s)}\n",
            "\n",
            ", where \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            " is allowed to change,\n",
            "dl\n",
            "p\n",
            "A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "V\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ")\n",
            "=\n",
            "\n",
            "max\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "\n",
            "E\n",
            "\n",
            "[\n",
            "G\n",
            "∣\n",
            "s\n",
            ",\n",
            "π\n",
            "]\n",
            "\n",
            "\n",
            "{\\displaystyle V^{*}(s)=\\max _{\\pi }\\mathbb {E} [G\\mid s,\\pi ]}\n",
            "\n",
            ", where \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            " is a state randomly sampled from the distribution \n",
            "\n",
            "\n",
            "\n",
            "μ\n",
            "\n",
            "\n",
            "{\\displaystyle \\mu }\n",
            "\n",
            " of initial states (so \n",
            "\n",
            "\n",
            "\n",
            "μ\n",
            "(\n",
            "s\n",
            ")\n",
            "=\n",
            "Pr\n",
            "(\n",
            "\n",
            "S\n",
            "\n",
            "0\n",
            "\n",
            "\n",
            "=\n",
            "s\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}\n",
            "\n",
            ").\n",
            "p\n",
            "Although state-values suffice to define optimality, it is useful to define action-values. Given a state \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            ", an action \n",
            "\n",
            "\n",
            "\n",
            "a\n",
            "\n",
            "\n",
            "{\\displaystyle a}\n",
            "\n",
            " and a policy \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            ", the action-value of the pair \n",
            "\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ",\n",
            "a\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle (s,a)}\n",
            "\n",
            " under \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            " is defined by\n",
            "dl\n",
            "p\n",
            "where \n",
            "\n",
            "\n",
            "\n",
            "G\n",
            "\n",
            "\n",
            "{\\displaystyle G}\n",
            "\n",
            " now stands for the random discounted return associated with first taking action \n",
            "\n",
            "\n",
            "\n",
            "a\n",
            "\n",
            "\n",
            "{\\displaystyle a}\n",
            "\n",
            " in state \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            " and following \n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi }\n",
            "\n",
            ", thereafter.\n",
            "p\n",
            "The theory of Markov decision processes states that if \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi ^{*}}\n",
            "\n",
            " is an optimal policy, we act optimally (take the optimal action) by choosing the action from \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ",\n",
            "⋅\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}\n",
            "\n",
            " with the highest action-value at each state, \n",
            "\n",
            "\n",
            "\n",
            "s\n",
            "\n",
            "\n",
            "{\\displaystyle s}\n",
            "\n",
            ". The action-value function of such an optimal policy (\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle Q^{\\pi ^{*}}}\n",
            "\n",
            ") is called the optimal action-value function and is commonly denoted by \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle Q^{*}}\n",
            "\n",
            ". In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\n",
            "p\n",
            "Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q\n",
            "\n",
            "k\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle Q_{k}}\n",
            "\n",
            " (\n",
            "\n",
            "\n",
            "\n",
            "k\n",
            "=\n",
            "0\n",
            ",\n",
            "1\n",
            ",\n",
            "2\n",
            ",\n",
            "…\n",
            "\n",
            "\n",
            "{\\displaystyle k=0,1,2,\\ldots }\n",
            "\n",
            ") that converge to \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q\n",
            "\n",
            "∗\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle Q^{*}}\n",
            "\n",
            ". Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading4\"><h4 id=\"Monte_Carlo_methods\">Monte Carlo methods</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=9\" title=\"Edit section: Monte Carlo methods\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "Monte Carlo methods[15] are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment’s dynamics, Monte Carlo methods rely solely on actual or simulated experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.\n",
            "p\n",
            "Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term “Monte Carlo” generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.\n",
            "p\n",
            "These methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process (MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.[14]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading4\"><h4 id=\"Temporal_difference_methods\">Temporal difference methods</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=10\" title=\"Edit section: Temporal difference methods\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "link\n",
            "div\n",
            "<div class=\"hatnote navigation-not-searchable\" role=\"note\">Main article: <a href=\"/wiki/Temporal_difference_learning\" title=\"Temporal difference learning\">Temporal difference learning</a></div>\n",
            "None\n",
            "p\n",
            "The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\n",
            "p\n",
            "The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[16][17] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[18] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\n",
            "p\n",
            "Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \n",
            "\n",
            "\n",
            "\n",
            "λ\n",
            "\n",
            "\n",
            "{\\displaystyle \\lambda }\n",
            "\n",
            " parameter \n",
            "\n",
            "\n",
            "\n",
            "(\n",
            "0\n",
            "≤\n",
            "λ\n",
            "≤\n",
            "1\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle (0\\leq \\lambda \\leq 1)}\n",
            "\n",
            " that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading4\"><h4 id=\"Function_approximation_methods\">Function approximation methods</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=11\" title=\"Edit section: Function approximation methods\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \n",
            "\n",
            "\n",
            "\n",
            "ϕ\n",
            "\n",
            "\n",
            "{\\displaystyle \\phi }\n",
            "\n",
            " that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \n",
            "\n",
            "\n",
            "\n",
            "(\n",
            "s\n",
            ",\n",
            "a\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle (s,a)}\n",
            "\n",
            " are obtained by linearly combining the components of \n",
            "\n",
            "\n",
            "\n",
            "ϕ\n",
            "(\n",
            "s\n",
            ",\n",
            "a\n",
            ")\n",
            "\n",
            "\n",
            "{\\displaystyle \\phi (s,a)}\n",
            "\n",
            " with some weights \n",
            "\n",
            "\n",
            "\n",
            "θ\n",
            "\n",
            "\n",
            "{\\displaystyle \\theta }\n",
            "\n",
            ":\n",
            "dl\n",
            "p\n",
            "The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\n",
            "p\n",
            "Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[19] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[20]\n",
            "p\n",
            "The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Direct_policy_search\">Direct policy search</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=12\" title=\"Edit section: Direct policy search\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\n",
            "p\n",
            "Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \n",
            "\n",
            "\n",
            "\n",
            "θ\n",
            "\n",
            "\n",
            "{\\displaystyle \\theta }\n",
            "\n",
            ", let \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "θ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle \\pi _{\\theta }}\n",
            "\n",
            " denote the policy associated to \n",
            "\n",
            "\n",
            "\n",
            "θ\n",
            "\n",
            "\n",
            "{\\displaystyle \\theta }\n",
            "\n",
            ". Defining the performance function by \n",
            "\n",
            "\n",
            "\n",
            "ρ\n",
            "(\n",
            "θ\n",
            ")\n",
            "=\n",
            "\n",
            "ρ\n",
            "\n",
            "\n",
            "π\n",
            "\n",
            "θ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}\n",
            "\n",
            " under mild conditions this function will be differentiable as a function of the parameter vector \n",
            "\n",
            "\n",
            "\n",
            "θ\n",
            "\n",
            "\n",
            "{\\displaystyle \\theta }\n",
            "\n",
            ". If the gradient of \n",
            "\n",
            "\n",
            "\n",
            "ρ\n",
            "\n",
            "\n",
            "{\\displaystyle \\rho }\n",
            "\n",
            " was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21] (which is known as the likelihood ratio method in the simulation-based optimization literature).[22]\n",
            "p\n",
            "A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\n",
            "p\n",
            "Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[23]\n",
            "p\n",
            "Policy search methods have been used in the robotics context.[24] Many policy search methods may get stuck in local optima (as they are based on local search).\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Model-based_algorithms\">Model-based algorithms</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=13\" title=\"Edit section: Model-based algorithms\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "Finally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26] to the learning algorithm.\n",
            "p\n",
            "Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]\n",
            "p\n",
            "There are other ways to use models than to update a value function.[28] For instance, in model predictive control the model is used to update the behavior directly.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Theory\">Theory</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=14\" title=\"Edit section: Theory\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Theory\">Theory</h2>\n",
            "p\n",
            "Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\n",
            "p\n",
            "Efficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\n",
            "p\n",
            "For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Research\">Research</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=15\" title=\"Edit section: Research\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Research\">Research</h2>\n",
            "style\n",
            "table\n",
            "p\n",
            "Research topics include:\n",
            "ul\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Comparison_of_key_algorithms\">Comparison of key algorithms</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=16\" title=\"Edit section: Comparison of key algorithms\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Comparison_of_key_algorithms\">Comparison of key algorithms</h2>\n",
            "table\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Associative_reinforcement_learning\">Associative reinforcement learning</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=17\" title=\"Edit section: Associative reinforcement learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[46]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Deep_reinforcement_learning\">Deep reinforcement learning</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=18\" title=\"Edit section: Deep reinforcement learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[47] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[48]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Adversarial_deep_reinforcement_learning\">Adversarial deep reinforcement learning</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=19\" title=\"Edit section: Adversarial deep reinforcement learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[49][50][51] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[52]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Fuzzy_reinforcement_learning\">Fuzzy reinforcement learning</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=20\" title=\"Edit section: Fuzzy reinforcement learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "By introducing fuzzy inference in reinforcement learning,[53] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [54] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Inverse_reinforcement_learning\">Inverse reinforcement learning</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=21\" title=\"Edit section: Inverse reinforcement learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[55] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [56] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [57] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\n",
            "div\n",
            "<div class=\"mw-heading mw-heading3\"><h3 id=\"Safe_reinforcement_learning\">Safe reinforcement learning</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=22\" title=\"Edit section: Safe reinforcement learning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "None\n",
            "p\n",
            "Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[58] An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the Conditional Value at Risk (CVaR).[59] In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.[60][61] However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias[62] and blindness to success.[63]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Statistical_comparison_of_reinforcement_learning_algorithms\">Statistical comparison of reinforcement learning algorithms</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=23\" title=\"Edit section: Statistical comparison of reinforcement learning algorithms\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Statistical_comparison_of_reinforcement_learning_algorithms\">Statistical comparison of reinforcement learning algorithms</h2>\n",
            "p\n",
            "Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[64] After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test.[65] This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[66]\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"See_also\">See also</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=24\" title=\"Edit section: See also\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"See_also\">See also</h2>\n",
            "style\n",
            "div\n",
            "<div class=\"div-col\" style=\"column-width: 20em;\">\n",
            "<ul><li><a href=\"/wiki/Temporal_difference_learning\" title=\"Temporal difference learning\">Temporal difference learning</a></li>\n",
            "<li><a href=\"/wiki/Q-learning\" title=\"Q-learning\">Q-learning</a></li>\n",
            "<li><a href=\"/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\" title=\"State–action–reward–state–action\">State–action–reward–state–action</a> (SARSA)</li>\n",
            "<li><a href=\"/wiki/Reinforcement_learning_from_human_feedback\" title=\"Reinforcement learning from human feedback\">Reinforcement learning from human feedback</a></li>\n",
            "<li><a href=\"/wiki/Optimal_control\" title=\"Optimal control\">Optimal control</a></li>\n",
            "<li><a href=\"/wiki/Error-driven_learning\" title=\"Error-driven learning\">Error-driven learning</a></li>\n",
            "<li><a href=\"/wiki/Multi-agent_reinforcement_learning\" title=\"Multi-agent reinforcement learning\">Multi-agent reinforcement learning</a></li>\n",
            "<li><a href=\"/wiki/Apprenticeship_learning\" title=\"Apprenticeship learning\">Apprenticeship learning</a></li>\n",
            "<li><a href=\"/wiki/Model-free_(reinforcement_learning)\" title=\"Model-free (reinforcement learning)\">Model-free (reinforcement learning)</a></li>\n",
            "<li><a class=\"new\" href=\"/w/index.php?title=Model-based_reinforcement_learning&amp;action=edit&amp;redlink=1\" title=\"Model-based reinforcement learning (page does not exist)\">Model-based reinforcement learning</a></li>\n",
            "<li><a class=\"new\" href=\"/w/index.php?title=Direct_reinforcement_learning&amp;action=edit&amp;redlink=1\" title=\"Direct reinforcement learning (page does not exist)\">Direct reinforcement learning</a><sup class=\"reference\" id=\"cite_ref-Li-2023_3-2\"><a href=\"#cite_note-Li-2023-3\"><span class=\"cite-bracket\">[</span>3<span class=\"cite-bracket\">]</span></a></sup> <sup class=\"reference\" id=\"cite_ref-Guan-2021_67-0\"><a href=\"#cite_note-Guan-2021-67\"><span class=\"cite-bracket\">[</span>67<span class=\"cite-bracket\">]</span></a></sup></li>\n",
            "<li><a class=\"new\" href=\"/w/index.php?title=Indirect_reinforcement_learning&amp;action=edit&amp;redlink=1\" title=\"Indirect reinforcement learning (page does not exist)\">Indirect reinforcement learning</a><sup class=\"reference\" id=\"cite_ref-Li-2023_3-3\"><a href=\"#cite_note-Li-2023-3\"><span class=\"cite-bracket\">[</span>3<span class=\"cite-bracket\">]</span></a></sup> <sup class=\"reference\" id=\"cite_ref-Guan-2021_67-1\"><a href=\"#cite_note-Guan-2021-67\"><span class=\"cite-bracket\">[</span>67<span class=\"cite-bracket\">]</span></a></sup></li>\n",
            "<li><a href=\"/wiki/Active_learning_(machine_learning)\" title=\"Active learning (machine learning)\">active learning (machine learning)</a></li></ul>\n",
            "</div>\n",
            "None\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"References\">References</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=25\" title=\"Edit section: References\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"References\">References</h2>\n",
            "style\n",
            "div\n",
            "<div class=\"reflist\">\n",
            "<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n",
            "<li id=\"cite_note-kaelbling-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-kaelbling_1-0\">^</a></b></span> <span class=\"reference-text\"><style data-mw-deduplicate=\"TemplateStyles:r1238218222\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite class=\"citation journal cs1\" id=\"CITEREFKaelblingLittmanMoore1996\"><a href=\"/wiki/Leslie_P._Kaelbling\" title=\"Leslie P. Kaelbling\">Kaelbling, Leslie P.</a>; <a href=\"/wiki/Michael_L._Littman\" title=\"Michael L. Littman\">Littman, Michael L.</a>; <a class=\"new\" href=\"/w/index.php?title=Andrew_W._Moore&amp;action=edit&amp;redlink=1\" title=\"Andrew W. Moore (page does not exist)\">Moore, Andrew W.</a> (1996). <a class=\"external text\" href=\"http://webarchive.loc.gov/all/20011120234539/http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html\" rel=\"nofollow\">\"Reinforcement Learning: A Survey\"</a>. <i>Journal of Artificial Intelligence Research</i>. <b>4</b>: 237–285. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/cs/9605103\" rel=\"nofollow\">cs/9605103</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1613%2Fjair.301\" rel=\"nofollow\">10.1613/jair.301</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:1708582\" rel=\"nofollow\">1708582</a>. Archived from <a class=\"external text\" href=\"http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html\" rel=\"nofollow\">the original</a> on 2001-11-20.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Reinforcement+Learning%3A+A+Survey&amp;rft.volume=4&amp;rft.pages=237-285&amp;rft.date=1996&amp;rft_id=info%3Aarxiv%2Fcs%2F9605103&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1708582%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.301&amp;rft.aulast=Kaelbling&amp;rft.aufirst=Leslie+P.&amp;rft.au=Littman%2C+Michael+L.&amp;rft.au=Moore%2C+Andrew+W.&amp;rft_id=http%3A%2F%2Fwww.cs.washington.edu%2Fresearch%2Fjair%2Fabstracts%2Fkaelbling96a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFvan_Otterlo,_M.Wiering,_M.2012\">van Otterlo, M.; Wiering, M. (2012). \"Reinforcement Learning and Markov Decision Processes\". <i>Reinforcement Learning</i>. Adaptation, Learning, and Optimization. Vol. 12. pp. 3–42. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-642-27645-3_1\" rel=\"nofollow\">10.1007/978-3-642-27645-3_1</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-642-27644-6\" title=\"Special:BookSources/978-3-642-27644-6\"><bdi>978-3-642-27644-6</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Reinforcement+Learning+and+Markov+Decision+Processes&amp;rft.btitle=Reinforcement+Learning&amp;rft.series=Adaptation%2C+Learning%2C+and+Optimization&amp;rft.pages=3-42&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-27645-3_1&amp;rft.isbn=978-3-642-27644-6&amp;rft.au=van+Otterlo%2C+M.&amp;rft.au=Wiering%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-Li-2023-3\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Li-2023_3-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Li-2023_3-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Li-2023_3-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Li-2023_3-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFLi2023\">Li, Shengbo (2023). <a class=\"external text\" href=\"https://link.springer.com/book/10.1007/978-981-19-7784-8\" rel=\"nofollow\"><i>Reinforcement Learning for Sequential Decision and Optimal Control</i></a> (First ed.). Springer Verlag, Singapore. pp. 1–460. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2F978-981-19-7784-8\" rel=\"nofollow\">10.1007/978-981-19-7784-8</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-9-811-97783-1\" title=\"Special:BookSources/978-9-811-97783-1\"><bdi>978-9-811-97783-1</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:257928563\" rel=\"nofollow\">257928563</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+Learning+for+Sequential+Decision+and+Optimal+Control&amp;rft.place=Springer+Verlag%2C+Singapore&amp;rft.pages=1-460&amp;rft.edition=First&amp;rft.date=2023&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A257928563%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-981-19-7784-8&amp;rft.isbn=978-9-811-97783-1&amp;rft.aulast=Li&amp;rft.aufirst=Shengbo&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fbook%2F10.1007%2F978-981-19-7784-8&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: location missing publisher (<a href=\"/wiki/Category:CS1_maint:_location_missing_publisher\" title=\"Category:CS1 maint: location missing publisher\">link</a>)</span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRussellNorvig2010\">Russell, Stuart J.; Norvig, Peter (2010). <i>Artificial intelligence : a modern approach</i> (Third ed.). Upper Saddle River, New Jersey. pp. 830, 831. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-13-604259-4\" title=\"Special:BookSources/978-0-13-604259-4\"><bdi>978-0-13-604259-4</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+intelligence+%3A+a+modern+approach&amp;rft.place=Upper+Saddle+River%2C+New+Jersey&amp;rft.pages=830%2C+831&amp;rft.edition=Third&amp;rft.date=2010&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: location missing publisher (<a href=\"/wiki/Category:CS1_maint:_location_missing_publisher\" title=\"Category:CS1 maint: location missing publisher\">link</a>)</span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFLeeSeoJung2012\">Lee, Daeyeol; Seo, Hyojung; Jung, Min Whan (21 July 2012). <a class=\"external text\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621\" rel=\"nofollow\">\"Neural Basis of Reinforcement Learning and Decision Making\"</a>. <i>Annual Review of Neuroscience</i>. <b>35</b> (1): 287–308. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1146%2Fannurev-neuro-062111-150512\" rel=\"nofollow\">10.1146/annurev-neuro-062111-150512</a>. <a class=\"mw-redirect\" href=\"/wiki/PMC_(identifier)\" title=\"PMC (identifier)\">PMC</a> <span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621\" rel=\"nofollow\">3490621</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/22462543\" rel=\"nofollow\">22462543</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annual+Review+of+Neuroscience&amp;rft.atitle=Neural+Basis+of+Reinforcement+Learning+and+Decision+Making&amp;rft.volume=35&amp;rft.issue=1&amp;rft.pages=287-308&amp;rft.date=2012-07-21&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3490621%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F22462543&amp;rft_id=info%3Adoi%2F10.1146%2Fannurev-neuro-062111-150512&amp;rft.aulast=Lee&amp;rft.aufirst=Daeyeol&amp;rft.au=Seo%2C+Hyojung&amp;rft.au=Jung%2C+Min+Whan&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3490621&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFSalazar_DuqueGiraldoVergaraNguyen2022\">Salazar Duque, Edgar Mauricio; Giraldo, Juan S.; Vergara, Pedro P.; Nguyen, Phuong; Van Der Molen, Anne; Slootweg, Han (2022). <a class=\"external text\" href=\"https://doi.org/10.1016%2Fj.epsr.2022.108515\" rel=\"nofollow\">\"Community energy storage operation via reinforcement learning with eligibility traces\"</a>. <i>Electric Power Systems Research</i>. <b>212</b>. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2022EPSR..21208515S\" rel=\"nofollow\">2022EPSR..21208515S</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.1016%2Fj.epsr.2022.108515\" rel=\"nofollow\">10.1016/j.epsr.2022.108515</a></span>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:250635151\" rel=\"nofollow\">250635151</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Electric+Power+Systems+Research&amp;rft.atitle=Community+energy+storage+operation+via+reinforcement+learning+with+eligibility+traces&amp;rft.volume=212&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A250635151%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1016%2Fj.epsr.2022.108515&amp;rft_id=info%3Abibcode%2F2022EPSR..21208515S&amp;rft.aulast=Salazar+Duque&amp;rft.aufirst=Edgar+Mauricio&amp;rft.au=Giraldo%2C+Juan+S.&amp;rft.au=Vergara%2C+Pedro+P.&amp;rft.au=Nguyen%2C+Phuong&amp;rft.au=Van+Der+Molen%2C+Anne&amp;rft.au=Slootweg%2C+Han&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.epsr.2022.108515&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFXieHung_Yu_LingNam_Hee_KimMichiel_van_de_Panne2020\">Xie, Zhaoming; Hung Yu Ling; Nam Hee Kim; Michiel van de Panne (2020). \"ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2005.04323\" rel=\"nofollow\">2005.04323</a></span> [<a class=\"external text\" href=\"https://arxiv.org/archive/cs.GR\" rel=\"nofollow\">cs.GR</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=ALLSTEPS%3A+Curriculum-driven+Learning+of+Stepping+Stone+Skills&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2005.04323&amp;rft.aulast=Xie&amp;rft.aufirst=Zhaoming&amp;rft.au=Hung+Yu+Ling&amp;rft.au=Nam+Hee+Kim&amp;rft.au=Michiel+van+de+Panne&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFVergaraSalazarGiraldoPalensky2022\">Vergara, Pedro P.; Salazar, Mauricio; Giraldo, Juan S.; Palensky, Peter (2022). <a class=\"external text\" href=\"https://doi.org/10.1016%2Fj.ijepes.2021.107628\" rel=\"nofollow\">\"Optimal dispatch of PV inverters in unbalanced distribution systems using Reinforcement Learning\"</a>. <i>International Journal of Electrical Power &amp; Energy Systems</i>. <b>136</b>. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2022IJEPE.13607628V\" rel=\"nofollow\">2022IJEPE.13607628V</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.1016%2Fj.ijepes.2021.107628\" rel=\"nofollow\">10.1016/j.ijepes.2021.107628</a></span>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:244099841\" rel=\"nofollow\">244099841</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Electrical+Power+%26+Energy+Systems&amp;rft.atitle=Optimal+dispatch+of+PV+inverters+in+unbalanced+distribution+systems+using+Reinforcement+Learning&amp;rft.volume=136&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A244099841%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijepes.2021.107628&amp;rft_id=info%3Abibcode%2F2022IJEPE.13607628V&amp;rft.aulast=Vergara&amp;rft.aufirst=Pedro+P.&amp;rft.au=Salazar%2C+Mauricio&amp;rft.au=Giraldo%2C+Juan+S.&amp;rft.au=Palensky%2C+Peter&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.ijepes.2021.107628&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-FOOTNOTESuttonBarto2018Chapter_11-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTESuttonBarto2018Chapter_11_9-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFSuttonBarto2018\">Sutton &amp; Barto 2018</a>, Chapter 11.</span>\n",
            "</li>\n",
            "<li id=\"cite_note-Ren-2022-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Ren-2022_10-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFRenJiangZhanLi2022\">Ren, Yangang; Jiang, Jianhua; Zhan, Guojian; Li, Shengbo Eben; Chen, Chen; Li, Keqiang; Duan, Jingliang (2022). <a class=\"external text\" href=\"https://ieeexplore.ieee.org/document/9857655\" rel=\"nofollow\">\"Self-Learned Intelligence for Integrated Decision and Control of Automated Vehicles at Signalized Intersections\"</a>. <i>IEEE Transactions on Intelligent Transportation Systems</i>. <b>23</b> (12): 24145–24156. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2110.12359\" rel=\"nofollow\">2110.12359</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1109%2FTITS.2022.3196167\" rel=\"nofollow\">10.1109/TITS.2022.3196167</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Intelligent+Transportation+Systems&amp;rft.atitle=Self-Learned+Intelligence+for+Integrated+Decision+and+Control+of+Automated+Vehicles+at+Signalized+Intersections&amp;rft.volume=23&amp;rft.issue=12&amp;rft.pages=24145-24156&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2110.12359&amp;rft_id=info%3Adoi%2F10.1109%2FTITS.2022.3196167&amp;rft.aulast=Ren&amp;rft.aufirst=Yangang&amp;rft.au=Jiang%2C+Jianhua&amp;rft.au=Zhan%2C+Guojian&amp;rft.au=Li%2C+Shengbo+Eben&amp;rft.au=Chen%2C+Chen&amp;rft.au=Li%2C+Keqiang&amp;rft.au=Duan%2C+Jingliang&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9857655&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFGosavi2003\"><a class=\"new\" href=\"/w/index.php?title=Abhijit_Gosavi&amp;action=edit&amp;redlink=1\" title=\"Abhijit Gosavi (page does not exist)\">Gosavi, Abhijit</a> (2003). <a class=\"external text\" href=\"https://www.springer.com/mathematics/applications/book/978-1-4020-7454-7\" rel=\"nofollow\"><i>Simulation-based Optimization: Parametric Optimization Techniques and Reinforcement</i></a>. Operations Research/Computer Science Interfaces Series. Springer. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-4020-7454-7\" title=\"Special:BookSources/978-1-4020-7454-7\"><bdi>978-1-4020-7454-7</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Simulation-based+Optimization%3A+Parametric+Optimization+Techniques+and+Reinforcement&amp;rft.series=Operations+Research%2FComputer+Science+Interfaces+Series&amp;rft.pub=Springer&amp;rft.date=2003&amp;rft.isbn=978-1-4020-7454-7&amp;rft.aulast=Gosavi&amp;rft.aufirst=Abhijit&amp;rft_id=https%3A%2F%2Fwww.springer.com%2Fmathematics%2Fapplications%2Fbook%2F978-1-4020-7454-7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-Optimal_adaptive_policies_for_Marko-12\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Optimal_adaptive_policies_for_Marko_12-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Optimal_adaptive_policies_for_Marko_12-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation cs2\" id=\"CITEREFBurnetasKatehakis1997\">Burnetas, Apostolos N.; <a class=\"mw-redirect\" href=\"/wiki/Michael_N._Katehakis\" title=\"Michael N. Katehakis\">Katehakis, Michael N.</a> (1997), \"Optimal adaptive policies for Markov Decision Processes\", <i><a href=\"/wiki/Mathematics_of_Operations_Research\" title=\"Mathematics of Operations Research\">Mathematics of Operations Research</a></i>, <b>22</b> (1): 222–255, <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1287%2Fmoor.22.1.222\" rel=\"nofollow\">10.1287/moor.22.1.222</a>, <a class=\"mw-redirect\" href=\"/wiki/JSTOR_(identifier)\" title=\"JSTOR (identifier)\">JSTOR</a> <a class=\"external text\" href=\"https://www.jstor.org/stable/3690147\" rel=\"nofollow\">3690147</a></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics+of+Operations+Research&amp;rft.atitle=Optimal+adaptive+policies+for+Markov+Decision+Processes&amp;rft.volume=22&amp;rft.issue=1&amp;rft.pages=222-255&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1287%2Fmoor.22.1.222&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F3690147%23id-name%3DJSTOR&amp;rft.aulast=Burnetas&amp;rft.aufirst=Apostolos+N.&amp;rft.au=Katehakis%2C+Michael+N.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation cs2\" id=\"CITEREFTokicPalm2011\">Tokic, Michel; Palm, Günther (2011), <a class=\"external text\" href=\"http://www.tokic.com/www/tokicm/publikationen/papers/KI2011.pdf\" rel=\"nofollow\">\"Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax\"</a> <span class=\"cs1-format\">(PDF)</span>, <i>KI 2011: Advances in Artificial Intelligence</i>, Lecture Notes in Computer Science, vol. 7006, Springer, pp. 335–346, <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-642-24455-1\" title=\"Special:BookSources/978-3-642-24455-1\"><bdi>978-3-642-24455-1</bdi></a></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Value-Difference+Based+Exploration%3A+Adaptive+Control+Between+Epsilon-Greedy+and+Softmax&amp;rft.btitle=KI+2011%3A+Advances+in+Artificial+Intelligence&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=335-346&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft.isbn=978-3-642-24455-1&amp;rft.aulast=Tokic&amp;rft.aufirst=Michel&amp;rft.au=Palm%2C+G%C3%BCnther&amp;rft_id=http%3A%2F%2Fwww.tokic.com%2Fwww%2Ftokicm%2Fpublikationen%2Fpapers%2FKI2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-:0-14\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:0_14-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:0_14-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:0_14-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\"><a class=\"external text\" href=\"https://web.archive.org/web/20170712170739/http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\" rel=\"nofollow\">\"Reinforcement learning: An introduction\"</a> <span class=\"cs1-format\">(PDF)</span>. Archived from <a class=\"external text\" href=\"http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\" rel=\"nofollow\">the original</a> <span class=\"cs1-format\">(PDF)</span> on 2017-07-12<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2017-07-23</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Reinforcement+learning%3A+An+introduction&amp;rft_id=http%3A%2F%2Fpeople.inf.elte.hu%2Florincz%2FFiles%2FRL_2006%2FSuttonBook.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFSinghSutton1996\">Singh, Satinder P.; Sutton, Richard S. (1996-03-01). <a class=\"external text\" href=\"https://link.springer.com/article/10.1007/BF00114726\" rel=\"nofollow\">\"Reinforcement learning with replacing eligibility traces\"</a>. <i>Machine Learning</i>. <b>22</b> (1): 123–158. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2FBF00114726\" rel=\"nofollow\">10.1007/BF00114726</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"https://search.worldcat.org/issn/1573-0565\" rel=\"nofollow\">1573-0565</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Reinforcement+learning+with+replacing+eligibility+traces&amp;rft.volume=22&amp;rft.issue=1&amp;rft.pages=123-158&amp;rft.date=1996-03-01&amp;rft_id=info%3Adoi%2F10.1007%2FBF00114726&amp;rft.issn=1573-0565&amp;rft.aulast=Singh&amp;rft.aufirst=Satinder+P.&amp;rft.au=Sutton%2C+Richard+S.&amp;rft_id=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2FBF00114726&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation thesis cs1\" id=\"CITEREFSutton1984\"><a href=\"/wiki/Richard_S._Sutton\" title=\"Richard S. Sutton\">Sutton, Richard S.</a> (1984). <a class=\"external text\" href=\"https://web.archive.org/web/20170330002227/http://incompleteideas.net/sutton/publications.html#PhDthesis\" rel=\"nofollow\"><i>Temporal Credit Assignment in Reinforcement Learning</i></a> (PhD thesis). University of Massachusetts, Amherst, MA. Archived from <a class=\"external text\" href=\"http://incompleteideas.net/sutton/publications.html#PhDthesis\" rel=\"nofollow\">the original</a> on 2017-03-30<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2017-03-29</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Temporal+Credit+Assignment+in+Reinforcement+Learning&amp;rft.degree=PhD&amp;rft.inst=University+of+Massachusetts%2C+Amherst%2C+MA&amp;rft.date=1984&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft_id=http%3A%2F%2Fincompleteideas.net%2Fsutton%2Fpublications.html%23PhDthesis&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]_17-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFSuttonBarto2018\">Sutton &amp; Barto 2018</a>, <a class=\"external text\" href=\"http://incompleteideas.net/sutton/book/ebook/node60.html\" rel=\"nofollow\">§6. Temporal-Difference Learning</a>.</span>\n",
            "</li>\n",
            "<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFBradtkeBarto1996\"><a class=\"new\" href=\"/w/index.php?title=Steven_J._Bradtke&amp;action=edit&amp;redlink=1\" title=\"Steven J. Bradtke (page does not exist)\">Bradtke, Steven J.</a>; <a class=\"mw-redirect\" href=\"/wiki/Andrew_G._Barto\" title=\"Andrew G. Barto\">Barto, Andrew G.</a> (1996). \"Learning to predict by the method of temporal differences\". <i>Machine Learning</i>. <b>22</b>: 33–57. <a class=\"mw-redirect\" href=\"/wiki/CiteSeerX_(identifier)\" title=\"CiteSeerX (identifier)\">CiteSeerX</a> <span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.143.857\" rel=\"nofollow\">10.1.1.143.857</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1023%2FA%3A1018056104778\" rel=\"nofollow\">10.1023/A:1018056104778</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:20327856\" rel=\"nofollow\">20327856</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Learning+to+predict+by+the+method+of+temporal+differences&amp;rft.volume=22&amp;rft.pages=33-57&amp;rft.date=1996&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.143.857%23id-name%3DCiteSeerX&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A20327856%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1018056104778&amp;rft.aulast=Bradtke&amp;rft.aufirst=Steven+J.&amp;rft.au=Barto%2C+Andrew+G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation thesis cs1\" id=\"CITEREFWatkins1989\"><a class=\"new\" href=\"/w/index.php?title=Christopher_J.C.H._Watkins&amp;action=edit&amp;redlink=1\" title=\"Christopher J.C.H. Watkins (page does not exist)\">Watkins, Christopher J.C.H.</a> (1989). <a class=\"external text\" href=\"http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf\" rel=\"nofollow\"><i>Learning from Delayed Rewards</i></a> <span class=\"cs1-format\">(PDF)</span> (PhD thesis). King’s College, Cambridge, UK.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Learning+from+Delayed+Rewards&amp;rft.degree=PhD&amp;rft.inst=King%E2%80%99s+College%2C+Cambridge%2C+UK&amp;rft.date=1989&amp;rft.aulast=Watkins&amp;rft.aufirst=Christopher+J.C.H.&amp;rft_id=http%3A%2F%2Fwww.cs.rhul.ac.uk%2F~chrisw%2Fnew_thesis.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-MBK-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-MBK_20-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFMatzliachBen-GalKagan2022\">Matzliach, Barouch; Ben-Gal, Irad; Kagan, Evgeny (2022). <a class=\"external text\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9407070\" rel=\"nofollow\">\"Detection of Static and Mobile Targets by an Autonomous Agent with Deep Q-Learning Abilities\"</a>. <i>Entropy</i>. <b>24</b> (8): 1168. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2022Entrp..24.1168M\" rel=\"nofollow\">2022Entrp..24.1168M</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.3390%2Fe24081168\" rel=\"nofollow\">10.3390/e24081168</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMC_(identifier)\" title=\"PMC (identifier)\">PMC</a> <span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9407070\" rel=\"nofollow\">9407070</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/36010832\" rel=\"nofollow\">36010832</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Entropy&amp;rft.atitle=Detection+of+Static+and+Mobile+Targets+by+an+Autonomous+Agent+with+Deep+Q-Learning+Abilities&amp;rft.volume=24&amp;rft.issue=8&amp;rft.pages=1168&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9407070%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F36010832&amp;rft_id=info%3Adoi%2F10.3390%2Fe24081168&amp;rft_id=info%3Abibcode%2F2022Entrp..24.1168M&amp;rft.aulast=Matzliach&amp;rft.aufirst=Barouch&amp;rft.au=Ben-Gal%2C+Irad&amp;rft.au=Kagan%2C+Evgeny&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9407070&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFWilliams1987\"><a href=\"/wiki/Ronald_J._Williams\" title=\"Ronald J. Williams\">Williams, Ronald J.</a> (1987). \"A class of gradient-estimating algorithms for reinforcement learning in neural networks\". <i>Proceedings of the IEEE First International Conference on Neural Networks</i>. <a class=\"mw-redirect\" href=\"/wiki/CiteSeerX_(identifier)\" title=\"CiteSeerX (identifier)\">CiteSeerX</a> <span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.8871\" rel=\"nofollow\">10.1.1.129.8871</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=A+class+of+gradient-estimating+algorithms+for+reinforcement+learning+in+neural+networks&amp;rft.btitle=Proceedings+of+the+IEEE+First+International+Conference+on+Neural+Networks&amp;rft.date=1987&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.129.8871%23id-name%3DCiteSeerX&amp;rft.aulast=Williams&amp;rft.aufirst=Ronald+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFPetersVijayakumarSchaal2003\"><a href=\"/wiki/Jan_Peters_(computer_scientist)\" title=\"Jan Peters (computer scientist)\">Peters, Jan</a>; <a href=\"/wiki/Sethu_Vijayakumar\" title=\"Sethu Vijayakumar\">Vijayakumar, Sethu</a>; <a href=\"/wiki/Stefan_Schaal\" title=\"Stefan Schaal\">Schaal, Stefan</a> (2003). <a class=\"external text\" href=\"http://web.archive.org/web/20130512223911/http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf\" rel=\"nofollow\"><i>Reinforcement Learning for Humanoid Robotics</i></a> <span class=\"cs1-format\">(PDF)</span>. IEEE-RAS International Conference on Humanoid Robots. Archived from <a class=\"external text\" href=\"http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf\" rel=\"nofollow\">the original</a> <span class=\"cs1-format\">(PDF)</span> on 2013-05-12.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Reinforcement+Learning+for+Humanoid+Robotics&amp;rft.date=2003&amp;rft.aulast=Peters&amp;rft.aufirst=Jan&amp;rft.au=Vijayakumar%2C+Sethu&amp;rft.au=Schaal%2C+Stefan&amp;rft_id=http%3A%2F%2Fwww-clmc.usc.edu%2Fpublications%2Fp%2Fpeters-ICHR2003.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFJuliani2016\">Juliani, Arthur (2016-12-17). <a class=\"external text\" href=\"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\" rel=\"nofollow\">\"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)\"</a>. <i>Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2018-02-22</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Simple+Reinforcement+Learning+with+Tensorflow+Part+8%3A+Asynchronous+Actor-Critic+Agents+%28A3C%29&amp;rft.date=2016-12-17&amp;rft.aulast=Juliani&amp;rft.aufirst=Arthur&amp;rft_id=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFDeisenrothNeumannPeters2013\"><a class=\"new\" href=\"/w/index.php?title=Marc_Peter_Deisenroth&amp;action=edit&amp;redlink=1\" title=\"Marc Peter Deisenroth (page does not exist)\">Deisenroth, Marc Peter</a>; <a href=\"/wiki/Gerhard_Neumann\" title=\"Gerhard Neumann\">Neumann, Gerhard</a>; <a href=\"/wiki/Jan_Peters_(computer_scientist)\" title=\"Jan Peters (computer scientist)\">Peters, Jan</a> (2013). <a class=\"external text\" href=\"http://eprints.lincoln.ac.uk/28029/1/PolicySearchReview.pdf\" rel=\"nofollow\"><i>A Survey on Policy Search for Robotics</i></a> <span class=\"cs1-format\">(PDF)</span>. Foundations and Trends in Robotics. Vol. 2. NOW Publishers. pp. 1–142. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1561%2F2300000021\" rel=\"nofollow\">10.1561/2300000021</a>. <a class=\"mw-redirect\" href=\"/wiki/Hdl_(identifier)\" title=\"Hdl (identifier)\">hdl</a>:<a class=\"external text\" href=\"https://hdl.handle.net/10044%2F1%2F12051\" rel=\"nofollow\">10044/1/12051</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Survey+on+Policy+Search+for+Robotics&amp;rft.series=Foundations+and+Trends+in+Robotics&amp;rft.pages=1-142&amp;rft.pub=NOW+Publishers&amp;rft.date=2013&amp;rft_id=info%3Ahdl%2F10044%2F1%2F12051&amp;rft_id=info%3Adoi%2F10.1561%2F2300000021&amp;rft.aulast=Deisenroth&amp;rft.aufirst=Marc+Peter&amp;rft.au=Neumann%2C+Gerhard&amp;rft.au=Peters%2C+Jan&amp;rft_id=http%3A%2F%2Feprints.lincoln.ac.uk%2F28029%2F1%2FPolicySearchReview.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFSutton1990\">Sutton, Richard (1990). \"Integrated Architectures for Learning, Planning and Reacting based on Dynamic Programming\". <i>Machine Learning: Proceedings of the Seventh International Workshop</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Integrated+Architectures+for+Learning%2C+Planning+and+Reacting+based+on+Dynamic+Programming&amp;rft.btitle=Machine+Learning%3A+Proceedings+of+the+Seventh+International+Workshop&amp;rft.date=1990&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFLin1992\">Lin, Long-Ji (1992). <a class=\"external text\" href=\"https://link.springer.com/content/pdf/10.1007/BF00992699.pdf\" rel=\"nofollow\">\"Self-improving reactive agents based on reinforcement learning, planning and teaching\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Machine Learning volume 8</i>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2FBF00992699\" rel=\"nofollow\">10.1007/BF00992699</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Self-improving+reactive+agents+based+on+reinforcement+learning%2C+planning+and+teaching&amp;rft.btitle=Machine+Learning+volume+8&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1007%2FBF00992699&amp;rft.aulast=Lin&amp;rft.aufirst=Long-Ji&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fcontent%2Fpdf%2F10.1007%2FBF00992699.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation cs2\" id=\"CITEREFZou2023\">Zou, Lan (2023-01-01), Zou, Lan (ed.), <a class=\"external text\" href=\"https://www.sciencedirect.com/science/article/pii/B9780323899314000110\" rel=\"nofollow\">\"Chapter 7 - Meta-reinforcement learning\"</a>, <i>Meta-Learning</i>, Academic Press, pp. 267–297, <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1016%2Fb978-0-323-89931-4.00011-0\" rel=\"nofollow\">10.1016/b978-0-323-89931-4.00011-0</a>, <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-323-89931-4\" title=\"Special:BookSources/978-0-323-89931-4\"><bdi>978-0-323-89931-4</bdi></a><span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">2023-11-08</span></span></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Meta-Learning&amp;rft.atitle=Chapter+7+-+Meta-reinforcement+learning&amp;rft.pages=267-297&amp;rft.date=2023-01-01&amp;rft_id=info%3Adoi%2F10.1016%2Fb978-0-323-89931-4.00011-0&amp;rft.isbn=978-0-323-89931-4&amp;rft.aulast=Zou&amp;rft.aufirst=Lan&amp;rft_id=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FB9780323899314000110&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFvan_HasseltHesselAslanides2019\">van Hasselt, Hado; Hessel, Matteo; Aslanides, John (2019). <a class=\"external text\" href=\"https://proceedings.neurips.cc/paper/2019/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf\" rel=\"nofollow\">\"When to use parametric models in reinforcement learning?\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Advances in Neural Information Processing Systems 32</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=When+to+use+parametric+models+in+reinforcement+learning%3F&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems+32&amp;rft.date=2019&amp;rft.aulast=van+Hasselt&amp;rft.aufirst=Hado&amp;rft.au=Hessel%2C+Matteo&amp;rft.au=Aslanides%2C+John&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2019%2Ffile%2F1b742ae215adf18b75449c6e272fd92d-Paper.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGrondmanVaandragerBusoniuBabuska2012\">Grondman, Ivo; Vaandrager, Maarten; Busoniu, Lucian; Babuska, Robert; Schuitema, Erik (2012-06-01). <a class=\"external text\" href=\"https://dl.acm.org/doi/10.1109/TSMCB.2011.2170565\" rel=\"nofollow\">\"Efficient Model Learning Methods for Actor–Critic Control\"</a>. <i>Trans. Sys. Man Cyber. Part B</i>. <b>42</b> (3): 591–602. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1109%2FTSMCB.2011.2170565\" rel=\"nofollow\">10.1109/TSMCB.2011.2170565</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"https://search.worldcat.org/issn/1083-4419\" rel=\"nofollow\">1083-4419</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Trans.+Sys.+Man+Cyber.+Part+B&amp;rft.atitle=Efficient+Model+Learning+Methods+for+Actor%E2%80%93Critic+Control&amp;rft.volume=42&amp;rft.issue=3&amp;rft.pages=591-602&amp;rft.date=2012-06-01&amp;rft_id=info%3Adoi%2F10.1109%2FTSMCB.2011.2170565&amp;rft.issn=1083-4419&amp;rft.aulast=Grondman&amp;rft.aufirst=Ivo&amp;rft.au=Vaandrager%2C+Maarten&amp;rft.au=Busoniu%2C+Lucian&amp;rft.au=Babuska%2C+Robert&amp;rft.au=Schuitema%2C+Erik&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1109%2FTSMCB.2011.2170565&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-30\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-30\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\"><a class=\"external text\" href=\"https://cie.acm.org/articles/use-reinforcements-learning-testing-game-mechanics/\" rel=\"nofollow\">\"On the Use of Reinforcement Learning for Testing Game Mechanics : ACM - Computers in Entertainment\"</a>. <i>cie.acm.org</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2018-11-27</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=cie.acm.org&amp;rft.atitle=On+the+Use+of+Reinforcement+Learning+for+Testing+Game+Mechanics+%3A+ACM+-+Computers+in+Entertainment&amp;rft_id=https%3A%2F%2Fcie.acm.org%2Farticles%2Fuse-reinforcements-learning-testing-game-mechanics%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFRiveretGao2019\">Riveret, Regis; Gao, Yang (2019). \"A probabilistic argumentation framework for reinforcement learning agents\". <i>Autonomous Agents and Multi-Agent Systems</i>. <b>33</b> (1–2): 216–274. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs10458-019-09404-2\" rel=\"nofollow\">10.1007/s10458-019-09404-2</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:71147890\" rel=\"nofollow\">71147890</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Autonomous+Agents+and+Multi-Agent+Systems&amp;rft.atitle=A+probabilistic+argumentation+framework+for+reinforcement+learning+agents&amp;rft.volume=33&amp;rft.issue=1%E2%80%932&amp;rft.pages=216-274&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1007%2Fs10458-019-09404-2&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A71147890%23id-name%3DS2CID&amp;rft.aulast=Riveret&amp;rft.aufirst=Regis&amp;rft.au=Gao%2C+Yang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFYamagataMcConvilleSantos-Rodriguez2021\">Yamagata, Taku; McConville, Ryan; Santos-Rodriguez, Raul (2021-11-16). \"Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2111.08596\" rel=\"nofollow\">2111.08596</a></span> [<a class=\"external text\" href=\"https://arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Reinforcement+Learning+with+Feedback+from+Multiple+Humans+with+Diverse+Skills&amp;rft.date=2021-11-16&amp;rft_id=info%3Aarxiv%2F2111.08596&amp;rft.aulast=Yamagata&amp;rft.aufirst=Taku&amp;rft.au=McConville%2C+Ryan&amp;rft.au=Santos-Rodriguez%2C+Raul&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKulkarniNarasimhanSaeediTenenbaum2016\">Kulkarni, Tejas D.; Narasimhan, Karthik R.; Saeedi, Ardavan; Tenenbaum, Joshua B. (2016). <a class=\"external text\" href=\"http://dl.acm.org/citation.cfm?id=3157382.3157509\" rel=\"nofollow\">\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\"</a>. <i>Proceedings of the 30th International Conference on Neural Information Processing Systems</i>. NIPS'16. USA: Curran Associates Inc.: 3682–3690. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1604.06057\" rel=\"nofollow\">1604.06057</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2016arXiv160406057K\" rel=\"nofollow\">2016arXiv160406057K</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-5108-3881-9\" title=\"Special:BookSources/978-1-5108-3881-9\"><bdi>978-1-5108-3881-9</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+30th+International+Conference+on+Neural+Information+Processing+Systems&amp;rft.atitle=Hierarchical+Deep+Reinforcement+Learning%3A+Integrating+Temporal+Abstraction+and+Intrinsic+Motivation&amp;rft.pages=3682-3690&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1604.06057&amp;rft_id=info%3Abibcode%2F2016arXiv160406057K&amp;rft.isbn=978-1-5108-3881-9&amp;rft.aulast=Kulkarni&amp;rft.aufirst=Tejas+D.&amp;rft.au=Narasimhan%2C+Karthik+R.&amp;rft.au=Saeedi%2C+Ardavan&amp;rft.au=Tenenbaum%2C+Joshua+B.&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D3157382.3157509&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\"><a class=\"external text\" href=\"http://umichrl.pbworks.com/Successes-of-Reinforcement-Learning/\" rel=\"nofollow\">\"Reinforcement Learning / Successes of Reinforcement Learning\"</a>. <i>umichrl.pbworks.com</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2017-08-06</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=umichrl.pbworks.com&amp;rft.atitle=Reinforcement+Learning+%2F+Successes+of+Reinforcement+Learning&amp;rft_id=http%3A%2F%2Fumichrl.pbworks.com%2FSuccesses-of-Reinforcement-Learning%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-35\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-35\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFDeySinghWangMcDonald-Maier2020\">Dey, Somdip; Singh, Amit Kumar; Wang, Xiaohang; McDonald-Maier, Klaus (March 2020). <a class=\"external text\" href=\"https://ieeexplore.ieee.org/document/9116294\" rel=\"nofollow\">\"User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs\"</a>. <a class=\"external text\" href=\"http://repository.essex.ac.uk/27546/1/User%20Interaction%20Aware%20Reinforcement%20Learning.pdf\" rel=\"nofollow\"><i>2020 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</i></a> <span class=\"cs1-format\">(PDF)</span>. pp. 1728–1733. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.23919%2FDATE48585.2020.9116294\" rel=\"nofollow\">10.23919/DATE48585.2020.9116294</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-9819263-4-7\" title=\"Special:BookSources/978-3-9819263-4-7\"><bdi>978-3-9819263-4-7</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:219858480\" rel=\"nofollow\">219858480</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=User+Interaction+Aware+Reinforcement+Learning+for+Power+and+Thermal+Efficiency+of+CPU-GPU+Mobile+MPSoCs&amp;rft.btitle=2020+Design%2C+Automation+%26+Test+in+Europe+Conference+%26+Exhibition+%28DATE%29&amp;rft.pages=1728-1733&amp;rft.date=2020-03&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A219858480%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.23919%2FDATE48585.2020.9116294&amp;rft.isbn=978-3-9819263-4-7&amp;rft.aulast=Dey&amp;rft.aufirst=Somdip&amp;rft.au=Singh%2C+Amit+Kumar&amp;rft.au=Wang%2C+Xiaohang&amp;rft.au=McDonald-Maier%2C+Klaus&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9116294&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-36\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-36\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFQuested\">Quested, Tony. <a class=\"external text\" href=\"https://www.businessweekly.co.uk/news/academia-research/smartphones-get-smarter-essex-innovation\" rel=\"nofollow\">\"Smartphones get smarter with Essex innovation\"</a>. <i>Business Weekly</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2021-06-17</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Business+Weekly&amp;rft.atitle=Smartphones+get+smarter+with+Essex+innovation&amp;rft.aulast=Quested&amp;rft.aufirst=Tony&amp;rft_id=https%3A%2F%2Fwww.businessweekly.co.uk%2Fnews%2Facademia-research%2Fsmartphones-get-smarter-essex-innovation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-37\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-37\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFWilliams2020\">Williams, Rhiannon (2020-07-21). <a class=\"external text\" href=\"https://inews.co.uk/news/technology/future-smartphones-prolong-battery-life-monitoring-behaviour-558689\" rel=\"nofollow\">\"Future smartphones 'will prolong their own battery life by monitoring owners' behaviour'<span class=\"cs1-kern-right\"></span>\"</a>. <i><a href=\"/wiki/I_(newspaper)\" title=\"I (newspaper)\">i</a></i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2021-06-17</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=i&amp;rft.atitle=Future+smartphones+%27will+prolong+their+own+battery+life+by+monitoring+owners%27+behaviour%27&amp;rft.date=2020-07-21&amp;rft.aulast=Williams&amp;rft.aufirst=Rhiannon&amp;rft_id=https%3A%2F%2Finews.co.uk%2Fnews%2Ftechnology%2Ffuture-smartphones-prolong-battery-life-monitoring-behaviour-558689&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-kaplan2004-38\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-kaplan2004_38-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFKaplanOudeyer2004\">Kaplan, F.; Oudeyer, P. (2004). \"Maximizing Learning Progress: An Internal Reward System for Development\". In Iida, F.; Pfeifer, R.; Steels, L.; Kuniyoshi, Y. (eds.). <i>Embodied Artificial Intelligence</i>. Lecture Notes in Computer Science. Vol. 3139. Berlin; Heidelberg: Springer. pp. 259–270. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-540-27833-7_19\" rel=\"nofollow\">10.1007/978-3-540-27833-7_19</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-540-22484-6\" title=\"Special:BookSources/978-3-540-22484-6\"><bdi>978-3-540-22484-6</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:9781221\" rel=\"nofollow\">9781221</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Maximizing+Learning+Progress%3A+An+Internal+Reward+System+for+Development&amp;rft.btitle=Embodied+Artificial+Intelligence&amp;rft.place=Berlin%3B+Heidelberg&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=259-270&amp;rft.pub=Springer&amp;rft.date=2004&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A9781221%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-540-27833-7_19&amp;rft.isbn=978-3-540-22484-6&amp;rft.aulast=Kaplan&amp;rft.aufirst=F.&amp;rft.au=Oudeyer%2C+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-klyubin2008-39\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-klyubin2008_39-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKlyubinPolaniNehaniv2008\">Klyubin, A.; Polani, D.; Nehaniv, C. (2008). <a class=\"external text\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607028\" rel=\"nofollow\">\"Keep your options open: an information-based driving principle for sensorimotor systems\"</a>. <i>PLOS ONE</i>. <b>3</b> (12): e4018. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2008PLoSO...3.4018K\" rel=\"nofollow\">2008PLoSO...3.4018K</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.1371%2Fjournal.pone.0004018\" rel=\"nofollow\">10.1371/journal.pone.0004018</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMC_(identifier)\" title=\"PMC (identifier)\">PMC</a> <span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607028\" rel=\"nofollow\">2607028</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/19107219\" rel=\"nofollow\">19107219</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+ONE&amp;rft.atitle=Keep+your+options+open%3A+an+information-based+driving+principle+for+sensorimotor+systems&amp;rft.volume=3&amp;rft.issue=12&amp;rft.pages=e4018&amp;rft.date=2008&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2607028%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F19107219&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pone.0004018&amp;rft_id=info%3Abibcode%2F2008PLoSO...3.4018K&amp;rft.aulast=Klyubin&amp;rft.aufirst=A.&amp;rft.au=Polani%2C+D.&amp;rft.au=Nehaniv%2C+C.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2607028&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-barto2013-40\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-barto2013_40-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFBarto2013\">Barto, A. G. (2013). \"Intrinsic motivation and reinforcement learning\". <a class=\"external text\" href=\"https://people.cs.umass.edu/~barto/IMCleVer-chapter-totypeset2.pdf\" rel=\"nofollow\"><i>Intrinsically Motivated Learning in Natural and Artificial Systems</i></a> <span class=\"cs1-format\">(PDF)</span>. Berlin; Heidelberg: Springer. pp. 17–47.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Intrinsic+motivation+and+reinforcement+learning&amp;rft.btitle=Intrinsically+Motivated+Learning+in+Natural+and+Artificial+Systems&amp;rft.place=Berlin%3B+Heidelberg&amp;rft.pages=17-47&amp;rft.pub=Springer&amp;rft.date=2013&amp;rft.aulast=Barto&amp;rft.aufirst=A.+G.&amp;rft_id=https%3A%2F%2Fpeople.cs.umass.edu%2F~barto%2FIMCleVer-chapter-totypeset2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-41\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-41\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFDabériusGranatKarlsson2020\">Dabérius, Kevin; Granat, Elvin; Karlsson, Patrik (2020). \"Deep Execution - Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks\". <i>The Journal of Machine Learning in Finance</i>. <b>1</b>. <a class=\"mw-redirect\" href=\"/wiki/SSRN_(identifier)\" title=\"SSRN (identifier)\">SSRN</a> <a class=\"external text\" href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3374766\" rel=\"nofollow\">3374766</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Machine+Learning+in+Finance&amp;rft.atitle=Deep+Execution+-+Value+and+Policy+Based+Reinforcement+Learning+for+Trading+and+Beating+Market+Benchmarks&amp;rft.volume=1&amp;rft.date=2020&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3374766%23id-name%3DSSRN&amp;rft.aulast=Dab%C3%A9rius&amp;rft.aufirst=Kevin&amp;rft.au=Granat%2C+Elvin&amp;rft.au=Karlsson%2C+Patrik&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-42\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-42\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGeorge_KarimpanalBouffanais2019\">George Karimpanal, Thommen; Bouffanais, Roland (2019). \"Self-organizing maps for storage and transfer of knowledge in reinforcement learning\". <i>Adaptive Behavior</i>. <b>27</b> (2): 111–126. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1811.08318\" rel=\"nofollow\">1811.08318</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1177%2F1059712318818568\" rel=\"nofollow\">10.1177/1059712318818568</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"https://search.worldcat.org/issn/1059-7123\" rel=\"nofollow\">1059-7123</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:53774629\" rel=\"nofollow\">53774629</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Adaptive+Behavior&amp;rft.atitle=Self-organizing+maps+for+storage+and+transfer+of+knowledge+in+reinforcement+learning&amp;rft.volume=27&amp;rft.issue=2&amp;rft.pages=111-126&amp;rft.date=2019&amp;rft_id=info%3Aarxiv%2F1811.08318&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A53774629%23id-name%3DS2CID&amp;rft.issn=1059-7123&amp;rft_id=info%3Adoi%2F10.1177%2F1059712318818568&amp;rft.aulast=George+Karimpanal&amp;rft.aufirst=Thommen&amp;rft.au=Bouffanais%2C+Roland&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-43\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-43\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFJ_DuanY_GuanS_Li2021\">J Duan; Y Guan; S Li (2021). <a class=\"external text\" href=\"https://ieeexplore.ieee.org/document/9448360\" rel=\"nofollow\">\"Distributional Soft Actor-Critic: Off-policy reinforcement learning for addressing value estimation errors\"</a>. <i>IEEE Transactions on Neural Networks and Learning Systems</i>. <b>33</b> (11): 6584–6598. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2001.02811\" rel=\"nofollow\">2001.02811</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1109%2FTNNLS.2021.3082568\" rel=\"nofollow\">10.1109/TNNLS.2021.3082568</a>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/34101599\" rel=\"nofollow\">34101599</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:211259373\" rel=\"nofollow\">211259373</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Neural+Networks+and+Learning+Systems&amp;rft.atitle=Distributional+Soft+Actor-Critic%3A+Off-policy+reinforcement+learning+for+addressing+value+estimation+errors&amp;rft.volume=33&amp;rft.issue=11&amp;rft.pages=6584-6598&amp;rft.date=2021&amp;rft_id=info%3Aarxiv%2F2001.02811&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A211259373%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F34101599&amp;rft_id=info%3Adoi%2F10.1109%2FTNNLS.2021.3082568&amp;rft.au=J+Duan&amp;rft.au=Y+Guan&amp;rft.au=S+Li&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9448360&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-44\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-44\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFY_RenJ_DuanS_Li2020\">Y Ren; J Duan; S Li (2020). <a class=\"external text\" href=\"https://ieeexplore.ieee.org/document/9294300\" rel=\"nofollow\">\"Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic\"</a>. <i>2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</i>. pp. 1–6. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2002.05502\" rel=\"nofollow\">2002.05502</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1109%2FITSC45102.2020.9294300\" rel=\"nofollow\">10.1109/ITSC45102.2020.9294300</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-7281-4149-7\" title=\"Special:BookSources/978-1-7281-4149-7\"><bdi>978-1-7281-4149-7</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:211096594\" rel=\"nofollow\">211096594</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Improving+Generalization+of+Reinforcement+Learning+with+Minimax+Distributional+Soft+Actor-Critic&amp;rft.btitle=2020+IEEE+23rd+International+Conference+on+Intelligent+Transportation+Systems+%28ITSC%29&amp;rft.pages=1-6&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2002.05502&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A211096594%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FITSC45102.2020.9294300&amp;rft.isbn=978-1-7281-4149-7&amp;rft.au=Y+Ren&amp;rft.au=J+Duan&amp;rft.au=S+Li&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9294300&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-45\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-45\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFDuanWangXiao2023\">Duan, J; Wang, W; Xiao, L (2023-10-26). \"DSAC-T: Distributional Soft Actor-Critic with Three Refinements\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2310.05858\" rel=\"nofollow\">2310.05858</a></span> [<a class=\"external text\" href=\"https://arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=DSAC-T%3A+Distributional+Soft+Actor-Critic+with+Three+Refinements&amp;rft.date=2023-10-26&amp;rft_id=info%3Aarxiv%2F2310.05858&amp;rft.aulast=Duan&amp;rft.aufirst=J&amp;rft.au=Wang%2C+W&amp;rft.au=Xiao%2C+L&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-46\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-46\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFSoucek1992\">Soucek, Branko (6 May 1992). <i>Dynamic, Genetic and Chaotic Programming: The Sixth-Generation Computer Technology Series</i>. John Wiley &amp; Sons, Inc. p. 38. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/0-471-55717-X\" title=\"Special:BookSources/0-471-55717-X\"><bdi>0-471-55717-X</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Dynamic%2C+Genetic+and+Chaotic+Programming%3A+The+Sixth-Generation+Computer+Technology+Series&amp;rft.pages=38&amp;rft.pub=John+Wiley+%26+Sons%2C+Inc&amp;rft.date=1992-05-06&amp;rft.isbn=0-471-55717-X&amp;rft.aulast=Soucek&amp;rft.aufirst=Branko&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-intro_deep_RL-47\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-intro_deep_RL_47-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFFrancois-Lavet2018\">Francois-Lavet, Vincent; et al. (2018). \"An Introduction to Deep Reinforcement Learning\". <i>Foundations and Trends in Machine Learning</i>. <b>11</b> (3–4): 219–354. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1811.12560\" rel=\"nofollow\">1811.12560</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2018arXiv181112560F\" rel=\"nofollow\">2018arXiv181112560F</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1561%2F2200000071\" rel=\"nofollow\">10.1561/2200000071</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:54434537\" rel=\"nofollow\">54434537</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=An+Introduction+to+Deep+Reinforcement+Learning&amp;rft.volume=11&amp;rft.issue=3%E2%80%934&amp;rft.pages=219-354&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1811.12560&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A54434537%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1561%2F2200000071&amp;rft_id=info%3Abibcode%2F2018arXiv181112560F&amp;rft.aulast=Francois-Lavet&amp;rft.aufirst=Vincent&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-DQN2-48\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-DQN2_48-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFMnih2015\">Mnih, Volodymyr; et al. (2015). \"Human-level control through deep reinforcement learning\". <i>Nature</i>. <b>518</b> (7540): 529–533. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2015Natur.518..529M\" rel=\"nofollow\">2015Natur.518..529M</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1038%2Fnature14236\" rel=\"nofollow\">10.1038/nature14236</a>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/25719670\" rel=\"nofollow\">25719670</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:205242740\" rel=\"nofollow\">205242740</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Human-level+control+through+deep+reinforcement+learning&amp;rft.volume=518&amp;rft.issue=7540&amp;rft.pages=529-533&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1038%2Fnature14236&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A205242740%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F25719670&amp;rft_id=info%3Abibcode%2F2015Natur.518..529M&amp;rft.aulast=Mnih&amp;rft.aufirst=Volodymyr&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-49\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-49\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGoodfellowShlensSzegedy2015\">Goodfellow, Ian; Shlens, Jonathan; Szegedy, Christian (2015). \"Explaining and Harnessing Adversarial Examples\". <i>International Conference on Learning Representations</i>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1412.6572\" rel=\"nofollow\">1412.6572</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Learning+Representations&amp;rft.atitle=Explaining+and+Harnessing+Adversarial+Examples&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1412.6572&amp;rft.aulast=Goodfellow&amp;rft.aufirst=Ian&amp;rft.au=Shlens%2C+Jonathan&amp;rft.au=Szegedy%2C+Christian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-50\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-50\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFBehzadanMunir2017\">Behzadan, Vahid; Munir, Arslan (2017). \"Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks\". <i>Machine Learning and Data Mining in Pattern Recognition</i>. Lecture Notes in Computer Science. Vol. 10358. pp. 262–275. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1701.04143\" rel=\"nofollow\">1701.04143</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-319-62416-7_19\" rel=\"nofollow\">10.1007/978-3-319-62416-7_19</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-319-62415-0\" title=\"Special:BookSources/978-3-319-62415-0\"><bdi>978-3-319-62415-0</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:1562290\" rel=\"nofollow\">1562290</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Vulnerability+of+Deep+Reinforcement+Learning+to+Policy+Induction+Attacks&amp;rft.btitle=Machine+Learning+and+Data+Mining+in+Pattern+Recognition&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=262-275&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1701.04143&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1562290%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-62416-7_19&amp;rft.isbn=978-3-319-62415-0&amp;rft.aulast=Behzadan&amp;rft.aufirst=Vahid&amp;rft.au=Munir%2C+Arslan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-51\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-51\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFPieter2017\">Pieter, Huang, Sandy Papernot, Nicolas Goodfellow, Ian Duan, Yan Abbeel (2017-02-07). <a class=\"external text\" href=\"http://worldcat.org/oclc/1106256905\" rel=\"nofollow\"><i>Adversarial Attacks on Neural Network Policies</i></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"https://search.worldcat.org/oclc/1106256905\" rel=\"nofollow\">1106256905</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Adversarial+Attacks+on+Neural+Network+Policies&amp;rft.date=2017-02-07&amp;rft_id=info%3Aoclcnum%2F1106256905&amp;rft.aulast=Pieter&amp;rft.aufirst=Huang%2C+Sandy+Papernot%2C+Nicolas+Goodfellow%2C+Ian+Duan%2C+Yan+Abbeel&amp;rft_id=http%3A%2F%2Fworldcat.org%2Foclc%2F1106256905&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">link</a>)</span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-52\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-52\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKorkmaz2022\">Korkmaz, Ezgi (2022). <a class=\"external text\" href=\"https://doi.org/10.1609%2Faaai.v36i7.20684\" rel=\"nofollow\">\"Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs\"</a>. <i>Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)</i>. <b>36</b> (7): 7229–7238. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2112.09025\" rel=\"nofollow\">2112.09025</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.1609%2Faaai.v36i7.20684\" rel=\"nofollow\">10.1609/aaai.v36i7.20684</a></span>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:245219157\" rel=\"nofollow\">245219157</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Thirty-Sixth+AAAI+Conference+on+Artificial+Intelligence+%28AAAI-22%29&amp;rft.atitle=Deep+Reinforcement+Learning+Policies+Learn+Shared+Adversarial+Features+Across+MDPs.&amp;rft.volume=36&amp;rft.issue=7&amp;rft.pages=7229-7238&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2112.09025&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A245219157%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1609%2Faaai.v36i7.20684&amp;rft.aulast=Korkmaz&amp;rft.aufirst=Ezgi&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1609%252Faaai.v36i7.20684&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-53\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-53\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFBerenji1994\">Berenji, H.R. (1994). <a class=\"external text\" href=\"https://ieeexplore.ieee.org/document/343737\" rel=\"nofollow\">\"Fuzzy Q-learning: A new approach for fuzzy dynamic programming\"</a>. <i>Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference</i>. Orlando, FL, USA: IEEE. pp. 486–491. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1109%2FFUZZY.1994.343737\" rel=\"nofollow\">10.1109/FUZZY.1994.343737</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/0-7803-1896-X\" title=\"Special:BookSources/0-7803-1896-X\"><bdi>0-7803-1896-X</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:56694947\" rel=\"nofollow\">56694947</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Fuzzy+Q-learning%3A+A+new+approach+for+fuzzy+dynamic+programming&amp;rft.btitle=Proceedings+of+1994+IEEE+3rd+International+Fuzzy+Systems+Conference&amp;rft.place=Orlando%2C+FL%2C+USA&amp;rft.pages=486-491&amp;rft.pub=IEEE&amp;rft.date=1994&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A56694947%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FFUZZY.1994.343737&amp;rft.isbn=0-7803-1896-X&amp;rft.aulast=Berenji&amp;rft.aufirst=H.R.&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F343737&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-54\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-54\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFVincze2017\">Vincze, David (2017). <a class=\"external text\" href=\"http://users.iit.uni-miskolc.hu/~vinczed/research/vinczed_sami2017_author_draft.pdf\" rel=\"nofollow\">\"Fuzzy rule interpolation and reinforcement learning\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>2017 IEEE 15th International Symposium on Applied Machine Intelligence and Informatics (SAMI)</i>. IEEE. pp. 173–178. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1109%2FSAMI.2017.7880298\" rel=\"nofollow\">10.1109/SAMI.2017.7880298</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-5090-5655-2\" title=\"Special:BookSources/978-1-5090-5655-2\"><bdi>978-1-5090-5655-2</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:17590120\" rel=\"nofollow\">17590120</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Fuzzy+rule+interpolation+and+reinforcement+learning&amp;rft.btitle=2017+IEEE+15th+International+Symposium+on+Applied+Machine+Intelligence+and+Informatics+%28SAMI%29&amp;rft.pages=173-178&amp;rft.pub=IEEE&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A17590120%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FSAMI.2017.7880298&amp;rft.isbn=978-1-5090-5655-2&amp;rft.aulast=Vincze&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Fusers.iit.uni-miskolc.hu%2F~vinczed%2Fresearch%2Fvinczed_sami2017_author_draft.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-55\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-55\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFNgRussell2000\">Ng, A. Y.; Russell, S. J. (2000). <a class=\"external text\" href=\"https://ai.stanford.edu/~ang/papers/icml00-irl.pdf\" rel=\"nofollow\">\"Algorithms for Inverse Reinforcement Learning\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Proceeding ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning</i>. pp. 663–670. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/1-55860-707-2\" title=\"Special:BookSources/1-55860-707-2\"><bdi>1-55860-707-2</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Algorithms+for+Inverse+Reinforcement+Learning&amp;rft.btitle=Proceeding+ICML+%2700+Proceedings+of+the+Seventeenth+International+Conference+on+Machine+Learning&amp;rft.pages=663-670&amp;rft.date=2000&amp;rft.isbn=1-55860-707-2&amp;rft.aulast=Ng&amp;rft.aufirst=A.+Y.&amp;rft.au=Russell%2C+S.+J.&amp;rft_id=https%3A%2F%2Fai.stanford.edu%2F~ang%2Fpapers%2Ficml00-irl.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-56\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-56\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFZiebartMaasBagnellDey2008\">Ziebart, Brian D.; Maas, Andrew; Bagnell, J. Andrew; Dey, Anind K. (2008-07-13). <a class=\"external text\" href=\"https://dl.acm.org/doi/10.5555/1620270.1620297\" rel=\"nofollow\">\"Maximum entropy inverse reinforcement learning\"</a>. <i>Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3</i>. AAAI'08. Chicago, Illinois: AAAI Press: 1433–1438. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-57735-368-3\" title=\"Special:BookSources/978-1-57735-368-3\"><bdi>978-1-57735-368-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:336219\" rel=\"nofollow\">336219</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+23rd+National+Conference+on+Artificial+Intelligence+-+Volume+3&amp;rft.atitle=Maximum+entropy+inverse+reinforcement+learning&amp;rft.pages=1433-1438&amp;rft.date=2008-07-13&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A336219%23id-name%3DS2CID&amp;rft.isbn=978-1-57735-368-3&amp;rft.aulast=Ziebart&amp;rft.aufirst=Brian+D.&amp;rft.au=Maas%2C+Andrew&amp;rft.au=Bagnell%2C+J.+Andrew&amp;rft.au=Dey%2C+Anind+K.&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.5555%2F1620270.1620297&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-57\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-57\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFPitombeira-NetoSantosCoelho_da_Silvade_Macedo2024\">Pitombeira-Neto, Anselmo R.; Santos, Helano P.; Coelho da Silva, Ticiana L.; de Macedo, José Antonio F. (March 2024). <a class=\"external text\" href=\"https://doi.org/10.1016/j.ins.2024.120128\" rel=\"nofollow\">\"Trajectory modeling via random utility inverse reinforcement learning\"</a>. <i>Information Sciences</i>. <b>660</b>: 120128. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2105.12092\" rel=\"nofollow\">2105.12092</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1016%2Fj.ins.2024.120128\" rel=\"nofollow\">10.1016/j.ins.2024.120128</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"https://search.worldcat.org/issn/0020-0255\" rel=\"nofollow\">0020-0255</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:235187141\" rel=\"nofollow\">235187141</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Sciences&amp;rft.atitle=Trajectory+modeling+via+random+utility+inverse+reinforcement+learning&amp;rft.volume=660&amp;rft.pages=120128&amp;rft.date=2024-03&amp;rft_id=info%3Aarxiv%2F2105.12092&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A235187141%23id-name%3DS2CID&amp;rft.issn=0020-0255&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ins.2024.120128&amp;rft.aulast=Pitombeira-Neto&amp;rft.aufirst=Anselmo+R.&amp;rft.au=Santos%2C+Helano+P.&amp;rft.au=Coelho+da+Silva%2C+Ticiana+L.&amp;rft.au=de+Macedo%2C+Jos%C3%A9+Antonio+F.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%2Fj.ins.2024.120128&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-58\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-58\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGarcíaFernández2015\">García, Javier; Fernández, Fernando (1 January 2015). <a class=\"external text\" href=\"https://jmlr.org/papers/volume16/garcia15a/garcia15a.pdf\" rel=\"nofollow\">\"A comprehensive survey on safe reinforcement learning\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>The Journal of Machine Learning Research</i>. <b>16</b> (1): 1437–1480.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Machine+Learning+Research&amp;rft.atitle=A+comprehensive+survey+on+safe+reinforcement+learning&amp;rft.volume=16&amp;rft.issue=1&amp;rft.pages=1437-1480&amp;rft.date=2015-01-01&amp;rft.aulast=Garc%C3%ADa&amp;rft.aufirst=Javier&amp;rft.au=Fern%C3%A1ndez%2C+Fernando&amp;rft_id=https%3A%2F%2Fjmlr.org%2Fpapers%2Fvolume16%2Fgarcia15a%2Fgarcia15a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-59\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-59\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFDabneyOstrovskiSilverMunos2018\">Dabney, Will; Ostrovski, Georg; Silver, David; Munos, Remi (2018-07-03). <a class=\"external text\" href=\"https://proceedings.mlr.press/v80/dabney18a.html\" rel=\"nofollow\">\"Implicit Quantile Networks for Distributional Reinforcement Learning\"</a>. <i>Proceedings of the 35th International Conference on Machine Learning</i>. PMLR: 1096–1105. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1806.06923\" rel=\"nofollow\">1806.06923</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+35th+International+Conference+on+Machine+Learning&amp;rft.atitle=Implicit+Quantile+Networks+for+Distributional+Reinforcement+Learning&amp;rft.pages=1096-1105&amp;rft.date=2018-07-03&amp;rft_id=info%3Aarxiv%2F1806.06923&amp;rft.aulast=Dabney&amp;rft.aufirst=Will&amp;rft.au=Ostrovski%2C+Georg&amp;rft.au=Silver%2C+David&amp;rft.au=Munos%2C+Remi&amp;rft_id=https%3A%2F%2Fproceedings.mlr.press%2Fv80%2Fdabney18a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-60\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-60\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFChowTamarMannorPavone2015\">Chow, Yinlam; Tamar, Aviv; Mannor, Shie; Pavone, Marco (2015). <a class=\"external text\" href=\"https://proceedings.neurips.cc/paper/2015/hash/64223ccf70bbb65a3a4aceac37e21016-Abstract.html\" rel=\"nofollow\">\"Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach\"</a>. <i>Advances in Neural Information Processing Systems</i>. <b>28</b>. Curran Associates, Inc. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1506.02188\" rel=\"nofollow\">1506.02188</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Risk-Sensitive+and+Robust+Decision-Making%3A+a+CVaR+Optimization+Approach&amp;rft.volume=28&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1506.02188&amp;rft.aulast=Chow&amp;rft.aufirst=Yinlam&amp;rft.au=Tamar%2C+Aviv&amp;rft.au=Mannor%2C+Shie&amp;rft.au=Pavone%2C+Marco&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2015%2Fhash%2F64223ccf70bbb65a3a4aceac37e21016-Abstract.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-61\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-61\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\"><a class=\"external text\" href=\"https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LnwyFkkAAAAJ&amp;citation_for_view=LnwyFkkAAAAJ:eQOLeE2rZwMC\" rel=\"nofollow\">\"Train Hard, Fight Easy: Robust Meta Reinforcement Learning\"</a>. <i>scholar.google.com</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2024-06-21</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=scholar.google.com&amp;rft.atitle=Train+Hard%2C+Fight+Easy%3A+Robust+Meta+Reinforcement+Learning&amp;rft_id=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DLnwyFkkAAAAJ%26citation_for_view%3DLnwyFkkAAAAJ%3AeQOLeE2rZwMC&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-62\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-62\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFTamarGlassnerMannor2015\">Tamar, Aviv; Glassner, Yonatan; Mannor, Shie (2015-02-21). <a class=\"external text\" href=\"https://ojs.aaai.org/index.php/AAAI/article/view/9561\" rel=\"nofollow\">\"Optimizing the CVaR via Sampling\"</a>. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>. <b>29</b> (1). <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1404.3862\" rel=\"nofollow\">1404.3862</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1609%2Faaai.v29i1.9561\" rel=\"nofollow\">10.1609/aaai.v29i1.9561</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"https://search.worldcat.org/issn/2374-3468\" rel=\"nofollow\">2374-3468</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence&amp;rft.atitle=Optimizing+the+CVaR+via+Sampling&amp;rft.volume=29&amp;rft.issue=1&amp;rft.date=2015-02-21&amp;rft_id=info%3Aarxiv%2F1404.3862&amp;rft.issn=2374-3468&amp;rft_id=info%3Adoi%2F10.1609%2Faaai.v29i1.9561&amp;rft.aulast=Tamar&amp;rft.aufirst=Aviv&amp;rft.au=Glassner%2C+Yonatan&amp;rft.au=Mannor%2C+Shie&amp;rft_id=https%3A%2F%2Fojs.aaai.org%2Findex.php%2FAAAI%2Farticle%2Fview%2F9561&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-63\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-63\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGreenbergChowGhavamzadehMannor2022\">Greenberg, Ido; Chow, Yinlam; Ghavamzadeh, Mohammad; Mannor, Shie (2022-12-06). <a class=\"external text\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d2511dfb731fa336739782ba825cd98c-Abstract-Conference.html\" rel=\"nofollow\">\"Efficient Risk-Averse Reinforcement Learning\"</a>. <i>Advances in Neural Information Processing Systems</i>. <b>35</b>: 32639–32652. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2205.05138\" rel=\"nofollow\">2205.05138</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Efficient+Risk-Averse+Reinforcement+Learning&amp;rft.volume=35&amp;rft.pages=32639-32652&amp;rft.date=2022-12-06&amp;rft_id=info%3Aarxiv%2F2205.05138&amp;rft.aulast=Greenberg&amp;rft.aufirst=Ido&amp;rft.au=Chow%2C+Yinlam&amp;rft.au=Ghavamzadeh%2C+Mohammad&amp;rft.au=Mannor%2C+Shie&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper_files%2Fpaper%2F2022%2Fhash%2Fd2511dfb731fa336739782ba825cd98c-Abstract-Conference.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-64\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-64\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFEngstromIlyasSanturkarTsipras2019\">Engstrom, Logan; Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Janoos, Firdaus; Rudolph, Larry; Madry, Aleksander (2019-09-25). <a class=\"external text\" href=\"https://openreview.net/forum?id=r1etN1rtPB\" rel=\"nofollow\">\"Implementation Matters in Deep RL: A Case Study on PPO and TRPO\"</a>. <i>ICLR</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=Implementation+Matters+in+Deep+RL%3A+A+Case+Study+on+PPO+and+TRPO&amp;rft.date=2019-09-25&amp;rft.aulast=Engstrom&amp;rft.aufirst=Logan&amp;rft.au=Ilyas%2C+Andrew&amp;rft.au=Santurkar%2C+Shibani&amp;rft.au=Tsipras%2C+Dimitris&amp;rft.au=Janoos%2C+Firdaus&amp;rft.au=Rudolph%2C+Larry&amp;rft.au=Madry%2C+Aleksander&amp;rft_id=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3Dr1etN1rtPB&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-65\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-65\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFColas2019\">Colas, Cédric (2019-03-06). <a class=\"external text\" href=\"https://openreview.net/forum?id=ryx0N3IaIV\" rel=\"nofollow\">\"A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms\"</a>. <i>International Conference on Learning Representations</i>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1904.06979\" rel=\"nofollow\">1904.06979</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Learning+Representations&amp;rft.atitle=A+Hitchhiker%27s+Guide+to+Statistical+Comparisons+of+Reinforcement+Learning+Algorithms&amp;rft.date=2019-03-06&amp;rft_id=info%3Aarxiv%2F1904.06979&amp;rft.aulast=Colas&amp;rft.aufirst=C%C3%A9dric&amp;rft_id=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3Dryx0N3IaIV&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-66\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-66\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGreenbergMannor2021\">Greenberg, Ido; Mannor, Shie (2021-07-01). <a class=\"external text\" href=\"https://proceedings.mlr.press/v139/greenberg21a.html\" rel=\"nofollow\">\"Detecting Rewards Deterioration in Episodic Reinforcement Learning\"</a>. <i>Proceedings of the 38th International Conference on Machine Learning</i>. PMLR: 3842–3853. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/2010.11660\" rel=\"nofollow\">2010.11660</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+38th+International+Conference+on+Machine+Learning&amp;rft.atitle=Detecting+Rewards+Deterioration+in+Episodic+Reinforcement+Learning&amp;rft.pages=3842-3853&amp;rft.date=2021-07-01&amp;rft_id=info%3Aarxiv%2F2010.11660&amp;rft.aulast=Greenberg&amp;rft.aufirst=Ido&amp;rft.au=Mannor%2C+Shie&amp;rft_id=https%3A%2F%2Fproceedings.mlr.press%2Fv139%2Fgreenberg21a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "<li id=\"cite_note-Guan-2021-67\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Guan-2021_67-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Guan-2021_67-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1238218222\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGuanLiDuan2021\">Guan, Yang; Li, Shengbo; Duan, Jiangliang (2021). <a class=\"external text\" href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22466\" rel=\"nofollow\">\"Direct and indirect reinforcement learning\"</a>. <i>International Journal of Intelligent Systems</i>. <b>36</b> (8): 4439–4467. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://arxiv.org/abs/1912.10600\" rel=\"nofollow\">1912.10600</a></span>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1002%2Fint.22466\" rel=\"nofollow\">10.1002/int.22466</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Intelligent+Systems&amp;rft.atitle=Direct+and+indirect+reinforcement+learning&amp;rft.volume=36&amp;rft.issue=8&amp;rft.pages=4439-4467&amp;rft.date=2021&amp;rft_id=info%3Aarxiv%2F1912.10600&amp;rft_id=info%3Adoi%2F10.1002%2Fint.22466&amp;rft.aulast=Guan&amp;rft.aufirst=Yang&amp;rft.au=Li%2C+Shengbo&amp;rft.au=Duan%2C+Jiangliang&amp;rft_id=https%3A%2F%2Fonlinelibrary.wiley.com%2Fdoi%2Fabs%2F10.1002%2Fint.22466&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning\"></span></span>\n",
            "</li>\n",
            "</ol></div></div>\n",
            "None\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Sources\">Sources</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=26\" title=\"Edit section: Sources\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Sources\">Sources</h2>\n",
            "ul\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"Further_reading\">Further reading</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=27\" title=\"Edit section: Further reading\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"Further_reading\">Further reading</h2>\n",
            "ul\n",
            "div\n",
            "<div class=\"mw-heading mw-heading2\"><h2 id=\"External_links\">External links</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=28\" title=\"Edit section: External links\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
            "<h2 id=\"External_links\">External links</h2>\n",
            "ul\n",
            "div\n",
            "<div class=\"navbox-styles\"><link href=\"mw-data:TemplateStyles:r1129693374\" rel=\"mw-deduplicated-inline-style\"/><style data-mw-deduplicate=\"TemplateStyles:r1236075235\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div>\n",
            "None\n",
            "div\n",
            "<div aria-labelledby=\"Differentiable_computing\" class=\"navbox\" role=\"navigation\" style=\"padding:3px\"><table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th class=\"navbox-title\" colspan=\"2\" scope=\"col\"><link href=\"mw-data:TemplateStyles:r1129693374\" rel=\"mw-deduplicated-inline-style\"/><link href=\"mw-data:TemplateStyles:r1239400231\" rel=\"mw-deduplicated-inline-style\"/><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Differentiable_computing\" title=\"Template:Differentiable computing\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Differentiable_computing\" title=\"Template talk:Differentiable computing\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a href=\"/wiki/Special:EditPage/Template:Differentiable_computing\" title=\"Special:EditPage/Template:Differentiable computing\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Differentiable_computing\" style=\"font-size:114%;margin:0 4em\">Differentiable computing</div></th></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Differentiable_function\" title=\"Differentiable function\">General</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><b><a href=\"/wiki/Differentiable_programming\" title=\"Differentiable programming\">Differentiable programming</a></b></li>\n",
            "<li><a href=\"/wiki/Information_geometry\" title=\"Information geometry\">Information geometry</a></li>\n",
            "<li><a href=\"/wiki/Statistical_manifold\" title=\"Statistical manifold\">Statistical manifold</a></li>\n",
            "<li><a href=\"/wiki/Automatic_differentiation\" title=\"Automatic differentiation\">Automatic differentiation</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Neuromorphic_engineering\" title=\"Neuromorphic engineering\">Neuromorphic engineering</a></li>\n",
            "<li><a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">Pattern recognition</a></li>\n",
            "<li><a href=\"/wiki/Tensor_calculus\" title=\"Tensor calculus\">Tensor calculus</a></li>\n",
            "<li><a href=\"/wiki/Computational_learning_theory\" title=\"Computational learning theory\">Computational learning theory</a></li>\n",
            "<li><a href=\"/wiki/Inductive_bias\" title=\"Inductive bias\">Inductive bias</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Concepts</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Gradient_descent\" title=\"Gradient descent\">Gradient descent</a>\n",
            "<ul><li><a href=\"/wiki/Stochastic_gradient_descent\" title=\"Stochastic gradient descent\">SGD</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">Clustering</a></li>\n",
            "<li><a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">Regression</a>\n",
            "<ul><li><a href=\"/wiki/Overfitting\" title=\"Overfitting\">Overfitting</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Hallucination_(artificial_intelligence)\" title=\"Hallucination (artificial intelligence)\">Hallucination</a></li>\n",
            "<li><a href=\"/wiki/Adversarial_machine_learning\" title=\"Adversarial machine learning\">Adversary</a></li>\n",
            "<li><a href=\"/wiki/Attention_(machine_learning)\" title=\"Attention (machine learning)\">Attention</a></li>\n",
            "<li><a href=\"/wiki/Convolution\" title=\"Convolution\">Convolution</a></li>\n",
            "<li><a href=\"/wiki/Loss_functions_for_classification\" title=\"Loss functions for classification\">Loss functions</a></li>\n",
            "<li><a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">Backpropagation</a></li>\n",
            "<li><a href=\"/wiki/Batch_normalization\" title=\"Batch normalization\">Batchnorm</a></li>\n",
            "<li><a href=\"/wiki/Activation_function\" title=\"Activation function\">Activation</a>\n",
            "<ul><li><a href=\"/wiki/Softmax_function\" title=\"Softmax function\">Softmax</a></li>\n",
            "<li><a href=\"/wiki/Sigmoid_function\" title=\"Sigmoid function\">Sigmoid</a></li>\n",
            "<li><a href=\"/wiki/Rectifier_(neural_networks)\" title=\"Rectifier (neural networks)\">Rectifier</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Regularization_(mathematics)\" title=\"Regularization (mathematics)\">Regularization</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Training,_validation,_and_test_sets\" title=\"Training, validation, and test sets\">Datasets</a>\n",
            "<ul><li><a href=\"/wiki/Data_augmentation\" title=\"Data augmentation\">Augmentation</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Diffusion_process\" title=\"Diffusion process\">Diffusion</a></li>\n",
            "<li><a href=\"/wiki/Autoregressive_model\" title=\"Autoregressive model\">Autoregression</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Applications</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a>\n",
            "<ul><li><a href=\"/wiki/Prompt_engineering#In-context_learning\" title=\"Prompt engineering\">In-context learning</a></li></ul></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">Artificial neural network</a>\n",
            "<ul><li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Computational_science\" title=\"Computational science\">Scientific computing</a></li>\n",
            "<li><a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">Artificial Intelligence</a></li>\n",
            "<li><a href=\"/wiki/Language_model\" title=\"Language model\">Language model</a>\n",
            "<ul><li><a href=\"/wiki/Large_language_model\" title=\"Large language model\">Large language model</a></li></ul></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Hardware</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Graphcore\" title=\"Graphcore\">IPU</a></li>\n",
            "<li><a href=\"/wiki/Tensor_Processing_Unit\" title=\"Tensor Processing Unit\">TPU</a></li>\n",
            "<li><a href=\"/wiki/Vision_processing_unit\" title=\"Vision processing unit\">VPU</a></li>\n",
            "<li><a href=\"/wiki/Memristor\" title=\"Memristor\">Memristor</a></li>\n",
            "<li><a href=\"/wiki/SpiNNaker\" title=\"SpiNNaker\">SpiNNaker</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Software libraries</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/TensorFlow\" title=\"TensorFlow\">TensorFlow</a></li>\n",
            "<li><a href=\"/wiki/PyTorch\" title=\"PyTorch\">PyTorch</a></li>\n",
            "<li><a href=\"/wiki/Keras\" title=\"Keras\">Keras</a></li>\n",
            "<li><a href=\"/wiki/Theano_(software)\" title=\"Theano (software)\">Theano</a></li>\n",
            "<li><a href=\"/wiki/Google_JAX\" title=\"Google JAX\">JAX</a></li>\n",
            "<li><a href=\"/wiki/Flux_(machine-learning_framework)\" title=\"Flux (machine-learning framework)\">Flux.jl</a></li>\n",
            "<li><a href=\"/wiki/MindSpore\" title=\"MindSpore\">MindSpore</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Implementations</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\"></div><table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"><tbody><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Audio–visual</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/AlexNet\" title=\"AlexNet\">AlexNet</a></li>\n",
            "<li><a href=\"/wiki/WaveNet\" title=\"WaveNet\">WaveNet</a></li>\n",
            "<li><a href=\"/wiki/Human_image_synthesis\" title=\"Human image synthesis\">Human image synthesis</a></li>\n",
            "<li><a href=\"/wiki/Handwriting_recognition\" title=\"Handwriting recognition\">HWR</a></li>\n",
            "<li><a href=\"/wiki/Optical_character_recognition\" title=\"Optical character recognition\">OCR</a></li>\n",
            "<li><a href=\"/wiki/Deep_learning_speech_synthesis\" title=\"Deep learning speech synthesis\">Speech synthesis</a></li>\n",
            "<li><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></li>\n",
            "<li><a href=\"/wiki/Facial_recognition_system\" title=\"Facial recognition system\">Facial recognition</a></li>\n",
            "<li><a href=\"/wiki/AlphaFold\" title=\"AlphaFold\">AlphaFold</a></li>\n",
            "<li><a href=\"/wiki/Text-to-image_model\" title=\"Text-to-image model\">Text-to-image models</a>\n",
            "<ul><li><a href=\"/wiki/DALL-E\" title=\"DALL-E\">DALL-E</a></li>\n",
            "<li><a href=\"/wiki/Midjourney\" title=\"Midjourney\">Midjourney</a></li>\n",
            "<li><a href=\"/wiki/Stable_Diffusion\" title=\"Stable Diffusion\">Stable Diffusion</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Text-to-video_model\" title=\"Text-to-video model\">Text-to-video models</a>\n",
            "<ul><li><a href=\"/wiki/Sora_(text-to-video_model)\" title=\"Sora (text-to-video model)\">Sora</a></li>\n",
            "<li><a href=\"/wiki/VideoPoet\" title=\"VideoPoet\">VideoPoet</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Whisper_(speech_recognition_system)\" title=\"Whisper (speech recognition system)\">Whisper</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Text</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Word2vec\" title=\"Word2vec\">Word2vec</a></li>\n",
            "<li><a href=\"/wiki/Seq2seq\" title=\"Seq2seq\">Seq2seq</a></li>\n",
            "<li><a href=\"/wiki/BERT_(language_model)\" title=\"BERT (language model)\">BERT</a></li>\n",
            "<li><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude</a></li>\n",
            "<li><a href=\"/wiki/Gemini_(language_model)\" title=\"Gemini (language model)\">Gemini</a></li>\n",
            "<li><a href=\"/wiki/LaMDA\" title=\"LaMDA\">LaMDA</a>\n",
            "<ul><li><a class=\"mw-redirect\" href=\"/wiki/Bard_(chatbot)\" title=\"Bard (chatbot)\">Bard</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Neural_machine_translation\" title=\"Neural machine translation\">NMT</a></li>\n",
            "<li><a href=\"/wiki/Project_Debater\" title=\"Project Debater\">Project Debater</a></li>\n",
            "<li><a href=\"/wiki/IBM_Watson\" title=\"IBM Watson\">IBM Watson</a></li>\n",
            "<li><a href=\"/wiki/IBM_Watsonx\" title=\"IBM Watsonx\">IBM Watsonx</a></li>\n",
            "<li><a href=\"/wiki/IBM_Granite\" title=\"IBM Granite\">Granite</a></li>\n",
            "<li><a href=\"/wiki/GPT-1\" title=\"GPT-1\">GPT-1</a></li>\n",
            "<li><a href=\"/wiki/GPT-2\" title=\"GPT-2\">GPT-2</a></li>\n",
            "<li><a href=\"/wiki/GPT-3\" title=\"GPT-3\">GPT-3</a></li>\n",
            "<li><a href=\"/wiki/GPT-4\" title=\"GPT-4\">GPT-4</a></li>\n",
            "<li><a href=\"/wiki/ChatGPT\" title=\"ChatGPT\">ChatGPT</a></li>\n",
            "<li><a href=\"/wiki/GPT-J\" title=\"GPT-J\">GPT-J</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Chinchilla_AI\" title=\"Chinchilla AI\">Chinchilla AI</a></li>\n",
            "<li><a href=\"/wiki/PaLM\" title=\"PaLM\">PaLM</a></li>\n",
            "<li><a href=\"/wiki/BLOOM_(language_model)\" title=\"BLOOM (language model)\">BLOOM</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/LLaMA\" title=\"LLaMA\">LLaMA</a></li>\n",
            "<li><a href=\"/wiki/Huawei_PanGu\" title=\"Huawei PanGu\">PanGu-Σ</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Decisional</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/AlphaGo\" title=\"AlphaGo\">AlphaGo</a></li>\n",
            "<li><a href=\"/wiki/AlphaZero\" title=\"AlphaZero\">AlphaZero</a></li>\n",
            "<li><a href=\"/wiki/Q-learning\" title=\"Q-learning\">Q-learning</a></li>\n",
            "<li><a href=\"/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\" title=\"State–action–reward–state–action\">SARSA</a></li>\n",
            "<li><a href=\"/wiki/OpenAI_Five\" title=\"OpenAI Five\">OpenAI Five</a></li>\n",
            "<li><a href=\"/wiki/Self-driving_car\" title=\"Self-driving car\">Self-driving car</a></li>\n",
            "<li><a href=\"/wiki/MuZero\" title=\"MuZero\">MuZero</a></li>\n",
            "<li><a href=\"/wiki/Action_selection\" title=\"Action selection\">Action selection</a>\n",
            "<ul><li><a href=\"/wiki/Auto-GPT\" title=\"Auto-GPT\">Auto-GPT</a></li></ul></li>\n",
            "<li><a href=\"/wiki/Robot_control\" title=\"Robot control\">Robot control</a></li></ul>\n",
            "</div></td></tr></tbody></table><div></div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">People</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Yoshua_Bengio\" title=\"Yoshua Bengio\">Yoshua Bengio</a></li>\n",
            "<li><a href=\"/wiki/Alex_Graves_(computer_scientist)\" title=\"Alex Graves (computer scientist)\">Alex Graves</a></li>\n",
            "<li><a href=\"/wiki/Ian_Goodfellow\" title=\"Ian Goodfellow\">Ian Goodfellow</a></li>\n",
            "<li><a href=\"/wiki/Stephen_Grossberg\" title=\"Stephen Grossberg\">Stephen Grossberg</a></li>\n",
            "<li><a href=\"/wiki/Demis_Hassabis\" title=\"Demis Hassabis\">Demis Hassabis</a></li>\n",
            "<li><a href=\"/wiki/Geoffrey_Hinton\" title=\"Geoffrey Hinton\">Geoffrey Hinton</a></li>\n",
            "<li><a href=\"/wiki/Yann_LeCun\" title=\"Yann LeCun\">Yann LeCun</a></li>\n",
            "<li><a href=\"/wiki/Fei-Fei_Li\" title=\"Fei-Fei Li\">Fei-Fei Li</a></li>\n",
            "<li><a href=\"/wiki/Andrew_Ng\" title=\"Andrew Ng\">Andrew Ng</a></li>\n",
            "<li><a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">Jürgen Schmidhuber</a></li>\n",
            "<li><a href=\"/wiki/David_Silver_(computer_scientist)\" title=\"David Silver (computer scientist)\">David Silver</a></li>\n",
            "<li><a href=\"/wiki/Ilya_Sutskever\" title=\"Ilya Sutskever\">Ilya Sutskever</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Organizations</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Anthropic\" title=\"Anthropic\">Anthropic</a></li>\n",
            "<li><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></li>\n",
            "<li><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a></li>\n",
            "<li><a href=\"/wiki/Hugging_Face\" title=\"Hugging Face\">Hugging Face</a></li>\n",
            "<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li>\n",
            "<li><a href=\"/wiki/Meta_AI\" title=\"Meta AI\">Meta AI</a></li>\n",
            "<li><a href=\"/wiki/Mila_(research_institute)\" title=\"Mila (research institute)\">Mila</a></li>\n",
            "<li><a href=\"/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory\" title=\"MIT Computer Science and Artificial Intelligence Laboratory\">MIT CSAIL</a></li>\n",
            "<li><a href=\"/wiki/Huawei\" title=\"Huawei\">Huawei</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Architectures</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Neural_Turing_machine\" title=\"Neural Turing machine\">Neural Turing machine</a></li>\n",
            "<li><a href=\"/wiki/Differentiable_neural_computer\" title=\"Differentiable neural computer\">Differentiable neural computer</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Transformer_(machine_learning_model)\" title=\"Transformer (machine learning model)\">Transformer</a></li>\n",
            "<li><a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">Recurrent neural network (RNN)</a></li>\n",
            "<li><a href=\"/wiki/Long_short-term_memory\" title=\"Long short-term memory\">Long short-term memory (LSTM)</a></li>\n",
            "<li><a href=\"/wiki/Gated_recurrent_unit\" title=\"Gated recurrent unit\">Gated recurrent unit (GRU)</a></li>\n",
            "<li><a href=\"/wiki/Echo_state_network\" title=\"Echo state network\">Echo state network</a></li>\n",
            "<li><a href=\"/wiki/Multilayer_perceptron\" title=\"Multilayer perceptron\">Multilayer perceptron (MLP)</a></li>\n",
            "<li><a href=\"/wiki/Convolutional_neural_network\" title=\"Convolutional neural network\">Convolutional neural network</a></li>\n",
            "<li><a href=\"/wiki/Residual_neural_network\" title=\"Residual neural network\">Residual neural network</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Mamba_(deep_learning)\" title=\"Mamba (deep learning)\">Mamba</a></li>\n",
            "<li><a href=\"/wiki/Autoencoder\" title=\"Autoencoder\">Autoencoder</a></li>\n",
            "<li><a href=\"/wiki/Variational_autoencoder\" title=\"Variational autoencoder\">Variational autoencoder (VAE)</a></li>\n",
            "<li><a href=\"/wiki/Generative_adversarial_network\" title=\"Generative adversarial network\">Generative adversarial network (GAN)</a></li>\n",
            "<li><a href=\"/wiki/Graph_neural_network\" title=\"Graph neural network\">Graph neural network</a></li></ul>\n",
            "</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div>\n",
            "<ul><li><span class=\"noviewer\" typeof=\"mw:File\"><a class=\"mw-file-description\" href=\"/wiki/File:Symbol_portal_class.svg\" title=\"Portal\"><img alt=\"\" class=\"mw-file-element\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/16px-Symbol_portal_class.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/23px-Symbol_portal_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/31px-Symbol_portal_class.svg.png 2x\" width=\"16\"/></a></span> Portals\n",
            "<ul><li><a href=\"/wiki/Portal:Computer_programming\" title=\"Portal:Computer programming\">Computer programming</a></li>\n",
            "<li><a href=\"/wiki/Portal:Technology\" title=\"Portal:Technology\">Technology</a></li></ul></li>\n",
            "<li><span class=\"noviewer\" typeof=\"mw:File\"><span title=\"Category\"><img alt=\"\" class=\"mw-file-element\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x\" width=\"16\"/></span></span> Categories\n",
            "<ul><li><a href=\"/wiki/Category:Artificial_neural_networks\" title=\"Category:Artificial neural networks\">Artificial neural networks</a></li>\n",
            "<li><a href=\"/wiki/Category:Machine_learning\" title=\"Category:Machine learning\">Machine learning</a></li></ul></li></ul>\n",
            "</div></td></tr></tbody></table></div>\n",
            "None\n",
            "div\n",
            "<div class=\"navbox-styles\"><link href=\"mw-data:TemplateStyles:r1129693374\" rel=\"mw-deduplicated-inline-style\"/><link href=\"mw-data:TemplateStyles:r1236075235\" rel=\"mw-deduplicated-inline-style\"/></div>\n",
            "None\n",
            "div\n",
            "<div aria-labelledby=\"Computer_science\" class=\"navbox\" role=\"navigation\" style=\"padding:3px\"><table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th class=\"navbox-title\" colspan=\"2\" scope=\"col\"><link href=\"mw-data:TemplateStyles:r1129693374\" rel=\"mw-deduplicated-inline-style\"/><link href=\"mw-data:TemplateStyles:r1239400231\" rel=\"mw-deduplicated-inline-style\"/><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Computer_science\" title=\"Template:Computer science\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Computer_science\" title=\"Template talk:Computer science\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a href=\"/wiki/Special:EditPage/Template:Computer_science\" title=\"Special:EditPage/Template:Computer science\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Computer_science\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Computer_science\" title=\"Computer science\">Computer science</a></div></th></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div>Note: This template roughly follows the 2012 <a href=\"/wiki/ACM_Computing_Classification_System\" title=\"ACM Computing Classification System\">ACM Computing Classification System</a>.</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Computer_hardware\" title=\"Computer hardware\">Hardware</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Printed_circuit_board\" title=\"Printed circuit board\">Printed circuit board</a></li>\n",
            "<li><a href=\"/wiki/Peripheral\" title=\"Peripheral\">Peripheral</a></li>\n",
            "<li><a href=\"/wiki/Integrated_circuit\" title=\"Integrated circuit\">Integrated circuit</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Very_Large_Scale_Integration\" title=\"Very Large Scale Integration\">Very Large Scale Integration</a></li>\n",
            "<li><a href=\"/wiki/System_on_a_chip\" title=\"System on a chip\">Systems on Chip (SoCs)</a></li>\n",
            "<li><a href=\"/wiki/Green_computing\" title=\"Green computing\">Energy consumption (Green computing)</a></li>\n",
            "<li><a href=\"/wiki/Electronic_design_automation\" title=\"Electronic design automation\">Electronic design automation</a></li>\n",
            "<li><a href=\"/wiki/Hardware_acceleration\" title=\"Hardware acceleration\">Hardware acceleration</a></li>\n",
            "<li><a href=\"/wiki/Processor_(computing)\" title=\"Processor (computing)\">Processor</a></li>\n",
            "<li><a href=\"/wiki/List_of_computer_size_categories\" title=\"List of computer size categories\">Size</a> / <a href=\"/wiki/Form_factor_(design)\" title=\"Form factor (design)\">Form</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Computer systems organization</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Computer_architecture\" title=\"Computer architecture\">Computer architecture</a></li>\n",
            "<li><a href=\"/wiki/Computational_complexity\" title=\"Computational complexity\">Computational complexity</a></li>\n",
            "<li><a href=\"/wiki/Dependability\" title=\"Dependability\">Dependability</a></li>\n",
            "<li><a href=\"/wiki/Embedded_system\" title=\"Embedded system\">Embedded system</a></li>\n",
            "<li><a href=\"/wiki/Real-time_computing\" title=\"Real-time computing\">Real-time computing</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Computer_network\" title=\"Computer network\">Networks</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Network_architecture\" title=\"Network architecture\">Network architecture</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Network_protocol\" title=\"Network protocol\">Network protocol</a></li>\n",
            "<li><a href=\"/wiki/Networking_hardware\" title=\"Networking hardware\">Network components</a></li>\n",
            "<li><a href=\"/wiki/Network_scheduler\" title=\"Network scheduler\">Network scheduler</a></li>\n",
            "<li><a href=\"/wiki/Network_performance\" title=\"Network performance\">Network performance evaluation</a></li>\n",
            "<li><a href=\"/wiki/Network_service\" title=\"Network service\">Network service</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Software organization</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Interpreter_(computing)\" title=\"Interpreter (computing)\">Interpreter</a></li>\n",
            "<li><a href=\"/wiki/Middleware\" title=\"Middleware\">Middleware</a></li>\n",
            "<li><a href=\"/wiki/Virtual_machine\" title=\"Virtual machine\">Virtual machine</a></li>\n",
            "<li><a href=\"/wiki/Operating_system\" title=\"Operating system\">Operating system</a></li>\n",
            "<li><a href=\"/wiki/Software_quality\" title=\"Software quality\">Software quality</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Programming_language_theory\" title=\"Programming language theory\">Software notations</a> and <a href=\"/wiki/Programming_tool\" title=\"Programming tool\">tools</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Programming_paradigm\" title=\"Programming paradigm\">Programming paradigm</a></li>\n",
            "<li><a href=\"/wiki/Programming_language\" title=\"Programming language\">Programming language</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Compiler_construction\" title=\"Compiler construction\">Compiler</a></li>\n",
            "<li><a href=\"/wiki/Domain-specific_language\" title=\"Domain-specific language\">Domain-specific language</a></li>\n",
            "<li><a href=\"/wiki/Modeling_language\" title=\"Modeling language\">Modeling language</a></li>\n",
            "<li><a href=\"/wiki/Software_framework\" title=\"Software framework\">Software framework</a></li>\n",
            "<li><a href=\"/wiki/Integrated_development_environment\" title=\"Integrated development environment\">Integrated development environment</a></li>\n",
            "<li><a href=\"/wiki/Software_configuration_management\" title=\"Software configuration management\">Software configuration management</a></li>\n",
            "<li><a href=\"/wiki/Library_(computing)\" title=\"Library (computing)\">Software library</a></li>\n",
            "<li><a href=\"/wiki/Software_repository\" title=\"Software repository\">Software repository</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Software_development\" title=\"Software development\">Software development</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a class=\"mw-redirect\" href=\"/wiki/Control_variable_(programming)\" title=\"Control variable (programming)\">Control variable</a></li>\n",
            "<li><a href=\"/wiki/Software_development_process\" title=\"Software development process\">Software development process</a></li>\n",
            "<li><a href=\"/wiki/Requirements_analysis\" title=\"Requirements analysis\">Requirements analysis</a></li>\n",
            "<li><a href=\"/wiki/Software_design\" title=\"Software design\">Software design</a></li>\n",
            "<li><a href=\"/wiki/Software_construction\" title=\"Software construction\">Software construction</a></li>\n",
            "<li><a href=\"/wiki/Software_deployment\" title=\"Software deployment\">Software deployment</a></li>\n",
            "<li><a href=\"/wiki/Software_engineering\" title=\"Software engineering\">Software engineering</a></li>\n",
            "<li><a href=\"/wiki/Software_maintenance\" title=\"Software maintenance\">Software maintenance</a></li>\n",
            "<li><a href=\"/wiki/Programming_team\" title=\"Programming team\">Programming team</a></li>\n",
            "<li><a href=\"/wiki/Open-source_software\" title=\"Open-source software\">Open-source model</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Theory_of_computation\" title=\"Theory of computation\">Theory of computation</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Model_of_computation\" title=\"Model of computation\">Model of computation</a></li>\n",
            "<li><a href=\"/wiki/Formal_language\" title=\"Formal language\">Formal language</a></li>\n",
            "<li><a href=\"/wiki/Automata_theory\" title=\"Automata theory\">Automata theory</a></li>\n",
            "<li><a href=\"/wiki/Computability_theory\" title=\"Computability theory\">Computability theory</a></li>\n",
            "<li><a href=\"/wiki/Computational_complexity_theory\" title=\"Computational complexity theory\">Computational complexity theory</a></li>\n",
            "<li><a href=\"/wiki/Logic_in_computer_science\" title=\"Logic in computer science\">Logic</a></li>\n",
            "<li><a href=\"/wiki/Semantics_(computer_science)\" title=\"Semantics (computer science)\">Semantics</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Algorithm\" title=\"Algorithm\">Algorithms</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a class=\"mw-redirect\" href=\"/wiki/Algorithm_design\" title=\"Algorithm design\">Algorithm design</a></li>\n",
            "<li><a href=\"/wiki/Analysis_of_algorithms\" title=\"Analysis of algorithms\">Analysis of algorithms</a></li>\n",
            "<li><a href=\"/wiki/Algorithmic_efficiency\" title=\"Algorithmic efficiency\">Algorithmic efficiency</a></li>\n",
            "<li><a href=\"/wiki/Randomized_algorithm\" title=\"Randomized algorithm\">Randomized algorithm</a></li>\n",
            "<li><a href=\"/wiki/Computational_geometry\" title=\"Computational geometry\">Computational geometry</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Mathematics of <a href=\"/wiki/Computing\" title=\"Computing\">computing</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Discrete_mathematics\" title=\"Discrete mathematics\">Discrete mathematics</a></li>\n",
            "<li><a href=\"/wiki/Probability\" title=\"Probability\">Probability</a></li>\n",
            "<li><a href=\"/wiki/Statistics\" title=\"Statistics\">Statistics</a></li>\n",
            "<li><a href=\"/wiki/Mathematical_software\" title=\"Mathematical software\">Mathematical software</a></li>\n",
            "<li><a href=\"/wiki/Information_theory\" title=\"Information theory\">Information theory</a></li>\n",
            "<li><a href=\"/wiki/Mathematical_analysis\" title=\"Mathematical analysis\">Mathematical analysis</a></li>\n",
            "<li><a href=\"/wiki/Numerical_analysis\" title=\"Numerical analysis\">Numerical analysis</a></li>\n",
            "<li><a href=\"/wiki/Theoretical_computer_science\" title=\"Theoretical computer science\">Theoretical computer science</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Information_system\" title=\"Information system\">Information systems</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Database\" title=\"Database\">Database management system</a></li>\n",
            "<li><a href=\"/wiki/Computer_data_storage\" title=\"Computer data storage\">Information storage systems</a></li>\n",
            "<li><a href=\"/wiki/Enterprise_information_system\" title=\"Enterprise information system\">Enterprise information system</a></li>\n",
            "<li><a href=\"/wiki/Social_software\" title=\"Social software\">Social information systems</a></li>\n",
            "<li><a href=\"/wiki/Geographic_information_system\" title=\"Geographic information system\">Geographic information system</a></li>\n",
            "<li><a href=\"/wiki/Decision_support_system\" title=\"Decision support system\">Decision support system</a></li>\n",
            "<li><a class=\"mw-redirect\" href=\"/wiki/Process_control\" title=\"Process control\">Process control system</a></li>\n",
            "<li><a href=\"/wiki/Multimedia_database\" title=\"Multimedia database\">Multimedia information system</a></li>\n",
            "<li><a href=\"/wiki/Data_mining\" title=\"Data mining\">Data mining</a></li>\n",
            "<li><a href=\"/wiki/Digital_library\" title=\"Digital library\">Digital library</a></li>\n",
            "<li><a href=\"/wiki/Computing_platform\" title=\"Computing platform\">Computing platform</a></li>\n",
            "<li><a href=\"/wiki/Digital_marketing\" title=\"Digital marketing\">Digital marketing</a></li>\n",
            "<li><a href=\"/wiki/World_Wide_Web\" title=\"World Wide Web\">World Wide Web</a></li>\n",
            "<li><a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">Information retrieval</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Computer_security\" title=\"Computer security\">Security</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Cryptography\" title=\"Cryptography\">Cryptography</a></li>\n",
            "<li><a href=\"/wiki/Formal_methods\" title=\"Formal methods\">Formal methods</a></li>\n",
            "<li><a href=\"/wiki/Security_hacker\" title=\"Security hacker\">Security hacker</a></li>\n",
            "<li><a href=\"/wiki/Security_service_(telecommunication)\" title=\"Security service (telecommunication)\">Security services</a></li>\n",
            "<li><a href=\"/wiki/Intrusion_detection_system\" title=\"Intrusion detection system\">Intrusion detection system</a></li>\n",
            "<li><a href=\"/wiki/Hardware_security\" title=\"Hardware security\">Hardware security</a></li>\n",
            "<li><a href=\"/wiki/Network_security\" title=\"Network security\">Network security</a></li>\n",
            "<li><a href=\"/wiki/Information_security\" title=\"Information security\">Information security</a></li>\n",
            "<li><a href=\"/wiki/Application_security\" title=\"Application security\">Application security</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Human%E2%80%93computer_interaction\" title=\"Human–computer interaction\">Human–computer interaction</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Interaction_design\" title=\"Interaction design\">Interaction design</a></li>\n",
            "<li><a href=\"/wiki/Social_computing\" title=\"Social computing\">Social computing</a></li>\n",
            "<li><a href=\"/wiki/Ubiquitous_computing\" title=\"Ubiquitous computing\">Ubiquitous computing</a></li>\n",
            "<li><a href=\"/wiki/Visualization_(graphics)\" title=\"Visualization (graphics)\">Visualization</a></li>\n",
            "<li><a href=\"/wiki/Computer_accessibility\" title=\"Computer accessibility\">Accessibility</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Concurrency_(computer_science)\" title=\"Concurrency (computer science)\">Concurrency</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Concurrent_computing\" title=\"Concurrent computing\">Concurrent computing</a></li>\n",
            "<li><a href=\"/wiki/Parallel_computing\" title=\"Parallel computing\">Parallel computing</a></li>\n",
            "<li><a href=\"/wiki/Distributed_computing\" title=\"Distributed computing\">Distributed computing</a></li>\n",
            "<li><a href=\"/wiki/Multithreading_(computer_architecture)\" title=\"Multithreading (computer architecture)\">Multithreading</a></li>\n",
            "<li><a href=\"/wiki/Multiprocessing\" title=\"Multiprocessing\">Multiprocessing</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">Artificial intelligence</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></li>\n",
            "<li><a href=\"/wiki/Knowledge_representation_and_reasoning\" title=\"Knowledge representation and reasoning\">Knowledge representation and reasoning</a></li>\n",
            "<li><a href=\"/wiki/Computer_vision\" title=\"Computer vision\">Computer vision</a></li>\n",
            "<li><a href=\"/wiki/Automated_planning_and_scheduling\" title=\"Automated planning and scheduling\">Automated planning and scheduling</a></li>\n",
            "<li><a href=\"/wiki/Mathematical_optimization\" title=\"Mathematical optimization\">Search methodology</a></li>\n",
            "<li><a href=\"/wiki/Control_theory\" title=\"Control theory\">Control method</a></li>\n",
            "<li><a href=\"/wiki/Philosophy_of_artificial_intelligence\" title=\"Philosophy of artificial intelligence\">Philosophy of artificial intelligence</a></li>\n",
            "<li><a href=\"/wiki/Distributed_artificial_intelligence\" title=\"Distributed artificial intelligence\">Distributed artificial intelligence</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Supervised_learning\" title=\"Supervised learning\">Supervised learning</a></li>\n",
            "<li><a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">Unsupervised learning</a></li>\n",
            "<li><a class=\"mw-selflink selflink\">Reinforcement learning</a></li>\n",
            "<li><a href=\"/wiki/Multi-task_learning\" title=\"Multi-task learning\">Multi-task learning</a></li>\n",
            "<li><a href=\"/wiki/Cross-validation_(statistics)\" title=\"Cross-validation (statistics)\">Cross-validation</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\"><a href=\"/wiki/Computer_graphics\" title=\"Computer graphics\">Graphics</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a href=\"/wiki/Computer_animation\" title=\"Computer animation\">Animation</a></li>\n",
            "<li><a href=\"/wiki/Rendering_(computer_graphics)\" title=\"Rendering (computer graphics)\">Rendering</a></li>\n",
            "<li><a href=\"/wiki/Photograph_manipulation\" title=\"Photograph manipulation\">Photograph manipulation</a></li>\n",
            "<li><a href=\"/wiki/Graphics_processing_unit\" title=\"Graphics processing unit\">Graphics processing unit</a></li>\n",
            "<li><a href=\"/wiki/Mixed_reality\" title=\"Mixed reality\">Mixed reality</a></li>\n",
            "<li><a href=\"/wiki/Virtual_reality\" title=\"Virtual reality\">Virtual reality</a></li>\n",
            "<li><a href=\"/wiki/Image_compression\" title=\"Image compression\">Image compression</a></li>\n",
            "<li><a href=\"/wiki/Solid_modeling\" title=\"Solid modeling\">Solid modeling</a></li></ul>\n",
            "</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Applied computing</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n",
            "<ul><li><a class=\"mw-redirect\" href=\"/wiki/Quantum_Computing\" title=\"Quantum Computing\">Quantum Computing</a></li>\n",
            "<li><a href=\"/wiki/E-commerce\" title=\"E-commerce\">E-commerce</a></li>\n",
            "<li><a href=\"/wiki/Enterprise_software\" title=\"Enterprise software\">Enterprise software</a></li>\n",
            "<li><a href=\"/wiki/Computational_mathematics\" title=\"Computational mathematics\">Computational mathematics</a></li>\n",
            "<li><a href=\"/wiki/Computational_physics\" title=\"Computational physics\">Computational physics</a></li>\n",
            "<li><a href=\"/wiki/Computational_chemistry\" title=\"Computational chemistry\">Computational chemistry</a></li>\n",
            "<li><a href=\"/wiki/Computational_biology\" title=\"Computational biology\">Computational biology</a></li>\n",
            "<li><a href=\"/wiki/Computational_social_science\" title=\"Computational social science\">Computational social science</a></li>\n",
            "<li><a href=\"/wiki/Computational_engineering\" title=\"Computational engineering\">Computational engineering</a></li>\n",
            "<li><a href=\"/wiki/Template:Differentiable_computing\" title=\"Template:Differentiable computing\">Differentiable computing</a></li>\n",
            "<li><a href=\"/wiki/Health_informatics\" title=\"Health informatics\">Computational healthcare</a></li>\n",
            "<li><a href=\"/wiki/Digital_art\" title=\"Digital art\">Digital art</a></li>\n",
            "<li><a href=\"/wiki/Electronic_publishing\" title=\"Electronic publishing\">Electronic publishing</a></li>\n",
            "<li><a href=\"/wiki/Cyberwarfare\" title=\"Cyberwarfare\">Cyberwarfare</a></li>\n",
            "<li><a href=\"/wiki/Electronic_voting\" title=\"Electronic voting\">Electronic voting</a></li>\n",
            "<li><a href=\"/wiki/Video_game\" title=\"Video game\">Video games</a></li>\n",
            "<li><a href=\"/wiki/Word_processor\" title=\"Word processor\">Word processing</a></li>\n",
            "<li><a href=\"/wiki/Operations_research\" title=\"Operations research\">Operations research</a></li>\n",
            "<li><a href=\"/wiki/Educational_technology\" title=\"Educational technology\">Educational technology</a></li>\n",
            "<li><a href=\"/wiki/Document_management_system\" title=\"Document management system\">Document management</a></li></ul>\n",
            "</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div>\n",
            "<ul><li><span class=\"noviewer\" typeof=\"mw:File\"><span title=\"Category\"><img alt=\"\" class=\"mw-file-element\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x\" width=\"16\"/></span></span> <a href=\"/wiki/Category:Computer_science\" title=\"Category:Computer science\">Category</a></li>\n",
            "<li><span class=\"noviewer\" typeof=\"mw:File\"><span title=\"Outline\"><img alt=\"\" class=\"mw-file-element\" data-file-height=\"200\" data-file-width=\"130\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/41/Global_thinking.svg/10px-Global_thinking.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/41/Global_thinking.svg/15px-Global_thinking.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/41/Global_thinking.svg/21px-Global_thinking.svg.png 2x\" width=\"10\"/></span></span> <a href=\"/wiki/Outline_of_computer_science\" title=\"Outline of computer science\">Outline</a></li>\n",
            "<li><span class=\"noviewer\" typeof=\"mw:File\"><span><img alt=\"\" class=\"mw-file-element\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e0/Symbol_question.svg/16px-Symbol_question.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e0/Symbol_question.svg/23px-Symbol_question.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e0/Symbol_question.svg/31px-Symbol_question.svg.png 2x\" width=\"16\"/></span></span> <a href=\"/wiki/Template:Glossaries_of_computers\" title=\"Template:Glossaries of computers\">Glossaries</a></li></ul>\n",
            "</div></td></tr></tbody></table></div>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# para_dict['Introduction'].strip()"
      ],
      "metadata": {
        "id": "uDAM1jR04Uhu"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sox7lRChE3qR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}